<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Phoenix W</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="./favicon.ico" />
       
<link rel="stylesheet" href="/blog/dist/main.css">

      
<link rel="stylesheet" href="/blog/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/blog/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="./images/cover6.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/blog/">Phoenix W</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['What can I help with?', '这个人很懒，博客里啥都没写', ''],
        startDelay: 0,
        typeSpeed: 70,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-第四周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/28/%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第四周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/28/%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-27T16:00:00.000Z" itemprop="datePublished">2024-12-28</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="逻辑斯蒂回归分类（Logistic-Regression）"><a href="#逻辑斯蒂回归分类（Logistic-Regression）" class="headerlink" title="逻辑斯蒂回归分类（Logistic Regression）"></a>逻辑斯蒂回归分类（Logistic Regression）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>对于某个样本 i，y<sub>k</sub> &#x3D; 1 如果该样本属于类别 k，否则 y<sub>k</sub> &#x3D; 0。</p>
<p>y^k 是模型预测的类别 k 的概率，通过<code>Softmax</code>函数计算得出。</p>
<p>K是类别数。<br>$$<br>L&#x3D;-\sum_{k&#x3D;1}^Ky_k⋅\ln(\hat{y_k})<br>$$<br>Z<sub>i,k</sub>&#x3D;X<sub>i</sub>W<sub>k</sub>+b<sub>k</sub> 是每个样本 i 的类别 k 的逻辑斯蒂加权和。</p>
<p>X<sub>i</sub>是样本 i 的输入特征，W<sub>k</sub>是类别 k 的权重，b<sub>k</sub>是类别 k 的偏置。<br>$$<br>\hat{y_k}&#x3D;\sum_{i&#x3D;1}^N{\frac{e^{Z_{i,k}}}{\sum_{j&#x3D;1}^Ke^{Z_{i,j}}}}<br>$$</p>
<p>$$<br>Z_{i,k}&#x3D;X_iW_k+b_k<br>$$</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><h4 id="通过将L对W求偏导数得出W的梯度："><a href="#通过将L对W求偏导数得出W的梯度：" class="headerlink" title="通过将L对W求偏导数得出W的梯度："></a>通过将L对W求偏导数得出W的梯度：</h4><p>$$<br>\frac{∂L_i}{∂W_k}&#x3D;\frac{∂L_i}{∂Z_{i,k}}⋅\frac{∂Z_{i,k}}{∂W_k}<br>$$</p>
<h5 id="第一项有："><a href="#第一项有：" class="headerlink" title="第一项有："></a>第一项有：</h5><p>$$<br>\frac{∂L_i}{∂Z_{i,k}}&#x3D;\frac{∂L_i}{∂\hat{y_{i,m}}}⋅\frac{∂\hat{y_{i,m}}}{∂Z_{i,k}}&#x3D;\sum_{m&#x3D;1}^K(-\frac{y_{i,m}}{\hat{y_{i,m}}})⋅\hat{y_{i,m}}(δ_{k.m}-\hat{y_{i,m})}&#x3D;\hat{y_{i,k}}-y_{i,k}<br>$$</p>
<p>δ<sub>k,m</sub>代表 k&#x3D;m 时取 1，否则为 0。</p>
<h5 id="第二项有："><a href="#第二项有：" class="headerlink" title="第二项有："></a>第二项有：</h5><p>$$<br>\frac{∂Z_{i,k}}{∂W_k}&#x3D;X_i<br>$$</p>
<h5 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h5><p>$$<br>\frac{∂L}{∂W_k}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(\hat{y_{i,k}}-y_{i,k})⋅X_i<br>$$</p>
<p>$$<br>\frac{∂L}{∂W}&#x3D;\frac{1}{N}X^T(\hat{y}-y)<br>$$</p>
<h4 id="通过将L对b求偏导数得出偏置b的梯度："><a href="#通过将L对b求偏导数得出偏置b的梯度：" class="headerlink" title="通过将L对b求偏导数得出偏置b的梯度："></a>通过将L对b求偏导数得出偏置b的梯度：</h4><h5 id="同理可得："><a href="#同理可得：" class="headerlink" title="同理可得："></a>同理可得：</h5><p>$$<br>\frac{∂L}{∂b}&#x3D;\frac{1}{N}(\hat{y}-y)<br>$$</p>
<h3 id="梯度下降："><a href="#梯度下降：" class="headerlink" title="梯度下降："></a>梯度下降：</h3><p>每次计算时，将现有权重和偏置减去梯度值，不断梯度下降。</p>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取Excel数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector, train_label = [], []</span><br><span class="line">    val_vector, val_label = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> (i % num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(val_vector), np.array(train_label), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="variable language_">self</span>.epochs = epochs</span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义softmax函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_softmax</span>(<span class="params">self, z</span>):</span><br><span class="line">        exp_z = np.exp(z - np.<span class="built_in">max</span>(z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">        <span class="keyword">return</span> exp_z / np.<span class="built_in">sum</span>(exp_z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义one_hot将标签转化为向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_one_hot</span>(<span class="params">self, label, num_classes</span>):</span><br><span class="line">        one_hot = np.zeros((label.shape[<span class="number">0</span>], num_classes))</span><br><span class="line">        one_hot[np.arange(label.shape[<span class="number">0</span>]), label] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型拟合函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, vector, label</span>):</span><br><span class="line">        num_samples, num_features = vector.shape</span><br><span class="line">        num_classes = np.<span class="built_in">max</span>(label) + <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.zeros((num_features, num_classes))</span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(num_classes)</span><br><span class="line">        y_one_hot = <span class="variable language_">self</span>._one_hot(label, num_classes)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        weight = np.zeros((num_features, num_classes))</span><br><span class="line">        <span class="comment"># 梯度下降循环</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.epochs):</span><br><span class="line">            <span class="keyword">if</span> weight.<span class="built_in">all</span>() == <span class="variable language_">self</span>.weights.<span class="built_in">all</span>():</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> count &gt; <span class="variable language_">self</span>.epochs // <span class="number">3</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                weight = <span class="variable language_">self</span>.weights</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">            logits = np.dot(vector, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">            y_pred = <span class="variable language_">self</span>._softmax(logits)</span><br><span class="line">            <span class="comment"># 计算W和b的偏导数</span></span><br><span class="line">            gradient_w = np.dot(vector.T, (y_pred - y_one_hot)) / num_samples</span><br><span class="line">            gradient_b = np.<span class="built_in">sum</span>(y_pred - y_one_hot, axis=<span class="number">0</span>) / num_samples</span><br><span class="line">            <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learning_rate * gradient_w</span><br><span class="line">            <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learning_rate * gradient_b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测效果</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, vector</span>):</span><br><span class="line">        logits = np.dot(vector, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        y_pred = <span class="variable language_">self</span>._softmax(logits)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = LogisticRegression(learning_rate=<span class="number">0.01</span>, epochs=<span class="number">50000</span>)</span><br><span class="line">    base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> item.is_file():</span><br><span class="line">            dataset_name = item.stem</span><br><span class="line">            vector, label = read_excel(item)</span><br><span class="line">            start = time.perf_counter()</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="built_in">all</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">                train_vector, val_vector, train_label, val_label = dataset_division(vector, label, num=<span class="number">10</span>, c=i)</span><br><span class="line">                model.fit(train_vector, train_label)</span><br><span class="line">                predictions = model.predict(val_vector)</span><br><span class="line">                count += np.<span class="built_in">sum</span>(predictions == val_label)</span><br><span class="line">                <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">            accuracy = count / <span class="built_in">all</span></span><br><span class="line">            end = time.perf_counter()</span><br><span class="line">            execution_time = end - start</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;dataset_name&#125;</span>`**, average accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;execution_time:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.05</span></span><br><span class="line">epoach = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 88.462%<br>Execution Time: 11.0502 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 45.540%<br>Execution Time: 6.8919 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 50.943%<br>Execution Time: 5.5206 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 97.315%<br>Execution Time: 5.1146 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 96.262%<br>Execution Time: 6.0894 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 72.881%<br>Execution Time: 7.5663 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 5.4938 seconds</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epoach = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 86.378%<br>Execution Time: 11.4022 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 47.418%<br>Execution Time: 6.9296 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 50.314%<br>Execution Time: 5.4587 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 97.315%<br>Execution Time: 5.2150 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 97.196%<br>Execution Time: 6.0267 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 72.881%<br>Execution Time: 7.4378 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 5.3925 seconds</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">epoach = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 89.423%<br>Execution Time: 11.3862 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 46.948%<br>Execution Time: 6.8464 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 51.572%<br>Execution Time: 5.3029 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 96.644%<br>Execution Time: 4.9203 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 97.196%<br>Execution Time: 6.4109 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 69.492%<br>Execution Time: 7.2088 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 5.2378 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><p>感觉<code>LearningRate</code>和<code>epoach</code>对准确度的影响好像没那么大。</p>
<p>当模型参数长时间不再优化时提前结束可以大大提高效率，并且对总体影响不大。</p>
<p><img src="https://raw.githubusercontent.com/CPhoenixW/blog/refs/heads/gh-pages/images/4-1.png" alt="4-1"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/%F0%9F%9A%80%F0%9F%9A%80%F0%9F%9A%80/" rel="tag">🚀🚀🚀</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第三周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/21/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第三周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/21/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-20T16:00:00.000Z" itemprop="datePublished">2024-12-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="朴素贝叶斯算法（Naive-Bayes）"><a href="#朴素贝叶斯算法（Naive-Bayes）" class="headerlink" title="朴素贝叶斯算法（Naive Bayes）"></a>朴素贝叶斯算法（Naive Bayes）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、验证集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="计算先验概率"><a href="#计算先验概率" class="headerlink" title="计算先验概率"></a>计算先验概率</h3><p>$$<br>P(C_K)&#x3D;\frac{|C_k|}{N}<br>$$</p>
<p>P(C<sub>k</sub>)为标签属于某一类别的概率。</p>
<h3 id="计算特征的似然值"><a href="#计算特征的似然值" class="headerlink" title="计算特征的似然值"></a>计算特征的似然值</h3><p>C<sub>k</sub>表示某一个标签，x表示某一个特征，d表示所取样本数量，P(x<sub>i</sub>|C<sub>k</sub>)表示某特征的每个特征值在C<sub>k</sub>下的概率。<br>$$<br>P(x|C_k)&#x3D;\prod_{i&#x3D;1}^d{P(x_i|C_k)}<br>$$<br>由于数据集中存在较多连续变量，不妨假设每个标签下的各个特征值满足正态分布。<code>sigma</code>代表代表标准差，<code>u</code>代表平均值。<br>$$<br>P(x_i|C_k)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-u)^2}{2\sigma^2}}<br>$$<br>P(x|C<sub>k</sub>)为每个特征的似然值。</p>
<h3 id="计算后验概率"><a href="#计算后验概率" class="headerlink" title="计算后验概率"></a>计算后验概率</h3><p>根据贝叶斯定理：<br>$$<br>P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}<br>$$<br>得出后验概率计算公式：</p>
<p>$$<br>P(C_k|x)&#x3D;\frac{P(C_k)P(x|C_k)}{P(A)}\propto P(C_k)P(x|C_k)<br>$$<br>带入之前计算的先验概率和似然值，选择后验概率最大的作为分类。<br>$$<br>\hat{C}&#x3D;arg\underset{C_k}{max}P(C_k|x)<br>$$</p>
<blockquote>
<p>⚠️⚠️⚠️</p>
<p>训练集负责计算模型参数：先验概率P(C<sub>k</sub>)、均值<code>u</code>、方差<code>sigma</code>。</p>
<p>测试机负责评估模型参数：利用模型参数计算似然值与后验概率。</p>
</blockquote>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取Excel数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> (i % num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(val_vector), np.array(train_label), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianNaiveBayes</span>:</span><br><span class="line">    <span class="comment"># 通过训练集构建模型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, vector, label</span>):</span><br><span class="line">        <span class="variable language_">self</span>.classes = np.unique(label)</span><br><span class="line">        <span class="variable language_">self</span>.preprob = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.means = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">vars</span> = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> values <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">            divided = vector[label == values]</span><br><span class="line">            <span class="variable language_">self</span>.preprob[values] = <span class="built_in">len</span>(divided) / <span class="built_in">len</span>(label)</span><br><span class="line">            <span class="variable language_">self</span>.means[values] = np.mean(divided, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">vars</span>[values] = np.var(divided, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">vars</span>[values][<span class="variable language_">self</span>.<span class="built_in">vars</span>[values] == <span class="number">0</span>] = <span class="number">1e-9</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测测试集分类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, labels</span>):</span><br><span class="line">        predictions = []</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            postprob = []</span><br><span class="line">            <span class="keyword">for</span> value <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">                prior = np.log(<span class="variable language_">self</span>.preprob[value])</span><br><span class="line">                likelihoods = <span class="variable language_">self</span>.cal_likelihood(label, value)</span><br><span class="line">                likelihoods[likelihoods == <span class="number">0</span>] = <span class="number">1e-9</span></span><br><span class="line">                likelihood = np.<span class="built_in">sum</span>(np.log(likelihoods))</span><br><span class="line">                postprob.append(prior + likelihood)</span><br><span class="line">            predictions.append(<span class="variable language_">self</span>.classes[np.argmax(postprob)])</span><br><span class="line">        <span class="keyword">return</span> np.array(predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算似然值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_likelihood</span>(<span class="params">self, value, label</span>):</span><br><span class="line">        mean = <span class="variable language_">self</span>.means[label]</span><br><span class="line">        var = <span class="variable language_">self</span>.<span class="built_in">vars</span>[label]</span><br><span class="line">        var = np.where(var == <span class="number">0</span>, <span class="number">1e-9</span>, var)</span><br><span class="line">        <span class="comment"># 正态分布公式</span></span><br><span class="line">        likelihood = (<span class="number">1</span> / np.sqrt(<span class="number">2</span> * np.pi * var)) * np.exp(-<span class="number">0.5</span> * ((value - mean) ** <span class="number">2</span>) / var) </span><br><span class="line">        likelihood[likelihood &lt; <span class="number">1e-9</span>] = <span class="number">1e-9</span></span><br><span class="line">        <span class="keyword">return</span> likelihood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = GaussianNaiveBayes()</span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        dataset_name = item.stem</span><br><span class="line">        vector, label = read_excel(item)</span><br><span class="line">        start = time.perf_counter()</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="built_in">all</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">            model.fit(train_vector, train_label)</span><br><span class="line">            predictions = model.predict(val_vector)</span><br><span class="line">            count += np.<span class="built_in">sum</span>(predictions == val_label)</span><br><span class="line">            <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">        accuracy = count / <span class="built_in">all</span></span><br><span class="line">        end = time.perf_counter()</span><br><span class="line">        execution_time = end - start</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;dataset_name&#125;</span>`**, average accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;execution_time:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 90.865%<br>Execution Time: 0.0236 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 46.948%<br>Execution Time: 0.0143 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 67.296%<br>Execution Time: 0.0062 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 95.302%<br>Execution Time: 0.0058 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 96.729%<br>Execution Time: 0.0080 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 98.305%<br>Execution Time: 0.0066 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 91.000%<br>Execution Time: 0.0093 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><p>在计算后验概率和似然值的时候选择了对每一组概率取对数，避免因数值过小而产生浮点数误差。</p>
<p><img src="https://github.com/CPhoenixW/blog/blob/gh-pages/images/2-1.png?raw=true" alt="2-1"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/wow/" rel="tag">wow!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第二周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/07/%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第二周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/07/%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-06T16:00:00.000Z" itemprop="datePublished">2024-12-07</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、验证集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="决策标准"><a href="#决策标准" class="headerlink" title="决策标准"></a>决策标准</h3><p>三种决策标准:</p>
<h4 id="信息增益值"><a href="#信息增益值" class="headerlink" title="信息增益值"></a>信息增益值</h4><p>D为全体样本，D<sub>i</sub>为某一特征下的各个取值，C<sub>k</sub>为各种标签，A为某一特征。由此可知n为特征下样本的取值个数，K为标签类别数。<br>$$<br>H(D) &#x3D; -\sum_{k&#x3D;1}^K \frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}<br>$$</p>
<p>$$<br>H(D|A)&#x3D;-\sum_{i&#x3D;1}^n \frac{|D_i|}{|D|}\sum_{k&#x3D;1}^K \frac{|D_{ik}|}{|D_i|}\log_2 \frac{|D_{ik}|}{|D_i|}<br>$$</p>
<p>$$<br>g(D,A)&#x3D;H(D)-H(D|A)<br>$$</p>
<p>得出信息增益值 g(D,A)</p>
<h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>适用于特征下取值较多时，用来避免高信息增益而低价值的判断。</p>
<p>$$<br>H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}<br>$$</p>
<p>$$<br>g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)}<br>$$</p>
<p>得出信息增益比 g<sub>R</sub>(D,A)</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>t某一特征，N<sub>i</sub>代表这个特征下不同标签类别，k为特征的总数。<br>$$<br>p_i&#x3D;\frac{|N_i|}{|N|}<br>$$</p>
<p>$$<br>Gini(t)&#x3D;1-\sum_{i&#x3D;1}^k p_i^2<br>$$</p>
<p>得出基尼指数 Gini(t)</p>
<h4 id="基尼分裂指数"><a href="#基尼分裂指数" class="headerlink" title="基尼分裂指数"></a>基尼分裂指数</h4><p>N<sub>L</sub>为待分裂的左子树，N<sub>R</sub>为待分裂的右子树。<br>$$<br>Gini_{split}&#x3D;\frac{N_L}{N_L+N_R}Gini(L)+\frac{N_L}{N_L+N_R}Gini(R)<br>$$<br>得出基尼分裂指数 Gini<sub>split</sub></p>
<h3 id="生成算法"><a href="#生成算法" class="headerlink" title="生成算法"></a>生成算法</h3><p>三种决策树生成算法：</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>以信息增益值为标准，选择信息增益值最大的维度为根节点，递归。</p>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>以信息增益比为标准，选择信息增益比最大的维度为根节点，递归。</p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>先根据选择基尼指数最小的作为根节点，再通过基尼分裂指数最小的选择分类方式，递归。</p>
<h3 id="剪枝算法"><a href="#剪枝算法" class="headerlink" title="剪枝算法"></a>剪枝算法</h3><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>当构建决策树时决策信息增益比过小时或基尼指数过大时，直接进行预剪枝，即把子树用用最多的标签代替。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>决策树构建完成后，自下而上进行扫描。若剪枝之后验证集的损失更小，就进行剪枝操作。</p>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="C4-5算法实现"><a href="#C4-5算法实现" class="headerlink" title="C4.5算法实现"></a>C4.5算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> i % num != c:</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(train_label), np.array(val_vector), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算熵值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_entropy</span>(<span class="params">label</span>):</span><br><span class="line">    a, counts = np.unique(label, return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(label)</span><br><span class="line">    entropy = -np.<span class="built_in">sum</span>(frac * np.log2(frac))</span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算信息增益比</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gain_ratio</span>(<span class="params">vector, label, i</span>):</span><br><span class="line">    <span class="comment"># 计算整体样本的熵值</span></span><br><span class="line">    entropy = cal_entropy(label)</span><br><span class="line">    di, counts = np.unique(vector[:, i], return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(vector)</span><br><span class="line">    gain = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(di)):</span><br><span class="line">        son = label[vector[:, i] == di[j]]</span><br><span class="line">        <span class="comment"># 计算某类标签的熵值</span></span><br><span class="line">        gain -= frac[j] * cal_entropy(son)</span><br><span class="line">    <span class="comment"># 计算固有值</span></span><br><span class="line">    IV = -np.<span class="built_in">sum</span>(frac * np.log2(frac))</span><br><span class="line">    <span class="keyword">if</span> IV == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    ratio = (entropy - gain) / IV</span><br><span class="line">    <span class="keyword">return</span> ratio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, label, epsilon=<span class="number">0.01</span>, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(label)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: label[<span class="number">0</span>]&#125;</span><br><span class="line">    <span class="keyword">if</span> vector.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    max_ratio, max_id = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vector[<span class="number">0</span>])):</span><br><span class="line">        ratio = cal_gain_ratio(vector, label, i)</span><br><span class="line">        <span class="keyword">if</span> ratio &gt; max_ratio:</span><br><span class="line">            max_ratio = ratio</span><br><span class="line">            max_id = i</span><br><span class="line">    <span class="comment"># 最大增益比小于阈值直接返回最多的类</span></span><br><span class="line">    <span class="keyword">if</span> max_ratio &lt; epsilon:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    values = np.unique(vector[:, max_id])</span><br><span class="line">    decision_tree = &#123;<span class="string">&#x27;point&#x27;</span>: max_id, <span class="string">&#x27;son&#x27;</span>: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> values:</span><br><span class="line">        son_vector = vector[vector[:, max_id] == value]</span><br><span class="line">        son_label = label[vector[:, max_id] == value]</span><br><span class="line">        son_vector = np.delete(son_vector, max_id, axis=<span class="number">1</span>)</span><br><span class="line">        decision_tree[<span class="string">&#x27;son&#x27;</span>][value] = build_tree(son_vector, son_label, epsilon, depth + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">decision_tree, value</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        point = decision_tree[<span class="string">&#x27;point&#x27;</span>]</span><br><span class="line">        feature_value = value[point]</span><br><span class="line">        <span class="keyword">if</span> feature_value <span class="keyword">in</span> decision_tree[<span class="string">&#x27;son&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][feature_value], value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.bincount(value.astype(<span class="built_in">int</span>)).argmax()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树剪枝</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cut_tree</span>(<span class="params">decision_tree, test_vector, test_label</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree</span><br><span class="line">    <span class="keyword">for</span> value, subtree <span class="keyword">in</span> decision_tree[<span class="string">&#x27;son&#x27;</span>].items():</span><br><span class="line">        decision_tree[<span class="string">&#x27;son&#x27;</span>][value] = cut_tree(subtree, test_vector, test_label)</span><br><span class="line">    post_label = np.bincount(test_label).argmax()</span><br><span class="line">    pre_error = cal_error(decision_tree, test_vector, test_label)</span><br><span class="line">    post_tree = &#123;<span class="string">&#x27;label&#x27;</span>: post_label&#125;</span><br><span class="line">    post_error = cal_error(post_tree, test_vector, test_label)</span><br><span class="line">    <span class="keyword">if</span> post_error &lt;= pre_error:</span><br><span class="line">        <span class="keyword">return</span> post_tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用测试集计算误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_error</span>(<span class="params">decision_tree, test_vector, test_label</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">        <span class="keyword">if</span> search_tree(decision_tree, test_vector[i]) != test_label[i]:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count / <span class="built_in">len</span>(test_vector)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        result.append(<span class="built_in">str</span>(item))</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">    vector, label = read_excel(<span class="built_in">dir</span>)</span><br><span class="line">    start = time.perf_counter()</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train_vector, train_label, test_vector, test_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">        train_vector, train_label, val_vector, val_label = dataset_division(train_vector, train_label, <span class="number">10</span>, i)</span><br><span class="line">        tree = build_tree(train_vector, train_label, epsilon=<span class="number">0.1</span>)</span><br><span class="line">        tree = cut_tree(tree, test_vector, test_label)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">            <span class="keyword">if</span> search_tree(tree, val_vector[j]) == val_label[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">10</span> / <span class="built_in">len</span>(val_label):<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    end = time.perf_counter()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 54.286%<br>Execution Time: 0.1993 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 34.211%<br>Execution Time: 0.3535 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 47.857%<br>Execution Time: 0.0382 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 33.846%<br>Execution Time: 0.0674 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 70.000%<br>Execution Time: 0.1220 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 39.375%<br>Execution Time: 0.4322 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 37.778%<br>Execution Time: 0.1743 seconds</p>
</blockquote>
<h3 id="CART算法复现"><a href="#CART算法复现" class="headerlink" title="CART算法复现"></a>CART算法复现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> i % num != c:</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(train_label), np.array(val_vector), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算基尼指数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gini</span>(<span class="params">label</span>):</span><br><span class="line">    _, counts = np.unique(label, return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(label)</span><br><span class="line">    gini = <span class="number">1</span> - np.<span class="built_in">sum</span>(frac ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> gini</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算最好的数据拆分点</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gini_index</span>(<span class="params">vector, label, i</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    best_gini = <span class="number">1000</span></span><br><span class="line">    best_split = <span class="literal">None</span></span><br><span class="line">    best_mask_left = <span class="literal">None</span></span><br><span class="line">    <span class="built_in">sorted</span> = np.sort(np.unique(vector[:, i]))</span><br><span class="line">    splits = (<span class="built_in">sorted</span>[:-<span class="number">1</span>] + <span class="built_in">sorted</span>[<span class="number">1</span>:]) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> splits:</span><br><span class="line">        left_mask, right_mask = vector[:, i] &lt;= split, vector[:, i] &gt; split</span><br><span class="line">        left_labels, right_labels = label[left_mask], label[right_mask]</span><br><span class="line">        left_gini, right_gini = cal_gini(left_labels), cal_gini(right_labels)</span><br><span class="line">        gini_index = (<span class="built_in">len</span>(left_labels) / n) * left_gini + (<span class="built_in">len</span>(right_labels) / n) * right_gini</span><br><span class="line">        <span class="keyword">if</span> gini_index &lt; best_gini:</span><br><span class="line">            best_gini = gini_index</span><br><span class="line">            best_split = split</span><br><span class="line">            best_mask_left = left_mask</span><br><span class="line">    <span class="keyword">if</span> best_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> best_gini, best_split, best_mask_left</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_gini, best_split, best_mask_left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, label, epsilon=<span class="number">0.01</span>, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(label)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: label[<span class="number">0</span>]&#125;</span><br><span class="line">    <span class="keyword">if</span> vector.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    min_gini_index = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">    best_feature = <span class="literal">None</span></span><br><span class="line">    best_split = <span class="literal">None</span></span><br><span class="line">    best_mask_left = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vector[<span class="number">0</span>])):</span><br><span class="line">        gini_index, split, mask_left = cal_gini_index(vector, label, i)</span><br><span class="line">        <span class="keyword">if</span> gini_index &lt; min_gini_index:</span><br><span class="line">            min_gini_index = gini_index</span><br><span class="line">            best_feature = i</span><br><span class="line">            best_split = split</span><br><span class="line">            best_mask_left = mask_left</span><br><span class="line">    <span class="comment"># 最佳拆分点基尼指数过大，直接用最多的标签替代</span></span><br><span class="line">    <span class="keyword">if</span> min_gini_index &gt;= epsilon:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    left_vector = vector[best_mask_left]</span><br><span class="line">    left_label = label[best_mask_left]</span><br><span class="line">    right_vector = vector[~best_mask_left]</span><br><span class="line">    right_label = label[~best_mask_left]</span><br><span class="line">    decision_tree = &#123;</span><br><span class="line">        <span class="string">&#x27;point&#x27;</span>: best_feature,</span><br><span class="line">        <span class="string">&#x27;split&#x27;</span>: best_split,</span><br><span class="line">        <span class="string">&#x27;son&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;left&#x27;</span>: build_tree(left_vector, left_label, epsilon, depth + <span class="number">1</span>),</span><br><span class="line">            <span class="string">&#x27;right&#x27;</span>: build_tree(right_vector, right_label, epsilon, depth + <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">decision_tree, value</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        point = decision_tree[<span class="string">&#x27;point&#x27;</span>]</span><br><span class="line">        feature_value = value[point]</span><br><span class="line">        <span class="keyword">if</span> feature_value &lt;= decision_tree[<span class="string">&#x27;split&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][<span class="string">&#x27;left&#x27;</span>], value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][<span class="string">&#x27;right&#x27;</span>], value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        result.append(<span class="built_in">str</span>(item))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">    vector, label = read_excel(<span class="built_in">dir</span>)</span><br><span class="line">    start = time.perf_counter()</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train_vector, train_label, test_vector, test_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">        train_vector, train_label, val_vector, val_label = dataset_division(train_vector, train_label, <span class="number">10</span>, i)</span><br><span class="line">        tree = build_tree(train_vector, train_label, epsilon=<span class="number">0.9</span>)</span><br><span class="line">        <span class="comment"># 神奇了，不剪枝效果竟然更好</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">            <span class="keyword">if</span> search_tree(tree, val_vector[j]) == val_label[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">10</span> / <span class="built_in">len</span>(val_label):<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    end = time.perf_counter()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong>(未后剪枝)</p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 77.857%<br>Execution Time: 0.1870 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 71.053%<br>Execution Time: 1.4938 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 84.286%<br>Execution Time: 0.0395 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 96.154%<br>Execution Time: 0.0880 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 94.211%<br>Execution Time: 0.3364 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 90.000%<br>Execution Time: 0.9370 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.667%<br>Execution Time: 0.0356 seconds</p>
</blockquote>
<p><strong>运行结果</strong>(经后剪枝)</p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 72.321%<br>Execution Time: 0.2086 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 43.684%<br>Execution Time: 1.3278 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 70.000%<br>Execution Time: 0.0362 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 84.615%<br>Execution Time: 0.0785 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 81.579%<br>Execution Time: 0.2871 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 71.250%<br>Execution Time: 0.8062 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 61.111%<br>Execution Time: 0.0316 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><h3 id="可视化效果"><a href="#可视化效果" class="headerlink" title="可视化效果"></a>可视化效果</h3><p><img src="https://raw.githubusercontent.com/CPhoenixW/blog/refs/heads/gh-pages/images/1-1.png" alt="1-1"></p>
<h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p>在CART算法中，将数据特征值当作连续的值先进行排序，后分段可能运用到了数据的连续特征信息，提高了识别的分类的准确率。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/wow/" rel="tag">wow!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第一周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/01/%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第一周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/01/%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-11-30T16:00:00.000Z" itemprop="datePublished">2024-12-01</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="K近邻算法（KNN）"><a href="#K近邻算法（KNN）" class="headerlink" title="K近邻算法（KNN）"></a>K近邻算法（KNN）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><p>两种距离计算方式：</p>
<p><strong>欧几里得距离</strong><br>$$<br>𝑑(𝑥,𝑦)&#x3D;\sqrt {\sum_{i&#x3D;1}^n (x_i-y_i)^2}<br>$$<br><strong>曼哈顿距离</strong><br>$$<br>d(x,y)&#x3D;\sum_{i&#x3D;1}^n \mid x_i -y_i \mid<br>$$</p>
<h3 id="顺序排列"><a href="#顺序排列" class="headerlink" title="顺序排列"></a>顺序排列</h3><p>根据之前计算的距离，选择k个距离最小的项进行计分。</p>
<h3 id="多数投票法"><a href="#多数投票法" class="headerlink" title="多数投票法"></a>多数投票法</h3><p>根据标签与计分表进行投票。判断最相似的结果。</p>
<h2 id="3-算法优化"><a href="#3-算法优化" class="headerlink" title="3.算法优化"></a>3.算法优化</h2><p>当训练集很大时，对顺序计算预测量与每一个标记点的距离非常耗时，采用<strong>KD树</strong><code>(K-dimensional tree)</code>提高效率。</p>
<h3 id="构建KD树"><a href="#构建KD树" class="headerlink" title="构建KD树"></a>构建KD树</h3><p><strong>1.选择一个维度</strong>，维度的选择通过循环决定（例如，选择第1维、2维、3维等，循环使用）来决定每一层分割的维度。</p>
<p><strong>2.选择分割点</strong>，将训练集中所选维度中位数定义为分割点，小于中位数归类于左子树，反之归类于右子树。</p>
<p><strong>3.递归分割</strong>，将左右两部分递归地进行相同的操作，直到分割到叶节点。</p>
<h3 id="查询最近邻"><a href="#查询最近邻" class="headerlink" title="查询最近邻"></a><strong>查询最近邻</strong></h3><p><strong>1.遍历树</strong>，从树的根节点开始，递归地遍历树。</p>
<p><strong>2.更新最短距离</strong>，若查询点到当前子树的根节点距离小于最小距离，更新最小距离。</p>
<p><strong>3.选择左子树或右子树</strong>，根据查询点的坐标与当前节点的分割维度进行比较，决定是否向左子树或右子树搜索。</p>
<ul>
<li>如果查询点再该维度下的坐标小于当前节点的分割点，则搜索左子树；反之，搜索右子树。</li>
</ul>
<p><strong>4.回溯并搜索另一子树</strong>，一旦进入一个子树，需要回溯并继续搜索另一子树。</p>
<p><strong>5.剪枝</strong>，如果当前子树中的端点到查询点的距离已经大于当前最短的距离，则跳过该子树的搜索。</p>
<h2 id="4-代码复现"><a href="#4-代码复现" class="headerlink" title="4.代码复现"></a>4.代码复现</h2><h3 id="全扫描版本"><a href="#全扫描版本" class="headerlink" title="全扫描版本"></a>全扫描版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span>(i%num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_vector, val_vector, train_label, val_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_distance</span>(<span class="params">train_vector, test_vector</span>):</span><br><span class="line">    distance = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_vector)):</span><br><span class="line">        count  = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">            count += (train_vector[i][j] - test_vector[j]) ** <span class="number">2</span></span><br><span class="line">        count = count ** <span class="number">0.5</span></span><br><span class="line">        distance.append(count)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sort_and_vote</span>(<span class="params">distance, train_label, k</span>):</span><br><span class="line">    vote_label = []</span><br><span class="line">    combine = <span class="built_in">list</span>(<span class="built_in">zip</span>(distance, train_label))</span><br><span class="line">    sort = <span class="built_in">sorted</span>(combine, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        vote_label.append(sort[i][<span class="number">1</span>])</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> vote_label:</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">in</span> label_count:</span><br><span class="line">            label_count[label] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label_count[label] = <span class="number">1</span></span><br><span class="line">    common = <span class="literal">None</span></span><br><span class="line">    <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> label, count <span class="keyword">in</span> label_count.items():</span><br><span class="line">        <span class="keyword">if</span> count &gt; <span class="built_in">max</span>:</span><br><span class="line">            common = label</span><br><span class="line">            <span class="built_in">max</span> = count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> common</span><br><span class="line"></span><br><span class="line">vector, label = read_excel(<span class="string">r&quot;E:\Document\MachineLearning\dataset\bal.xls&quot;</span>)</span><br><span class="line">start = time.perf_counter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">        test_vector = val_vector[j]</span><br><span class="line">        distance = cal_distance(train_vector, test_vector)</span><br><span class="line">        common = sort_and_vote(distance, train_label, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span>(common == val_label[j]):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation %d, accuracy: %.3f %%&quot;</span>%(i, count / <span class="built_in">len</span>(val_vector)*<span class="number">100</span>))</span><br><span class="line">end = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running spend %f s&quot;</span>%(end - start))</span><br></pre></td></tr></table></figure>

<h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果:"></a>运行结果:</h4><p><code>bal</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 92.063 %<br>Validation 1, accuracy: 79.365 %<br>Validation 2, accuracy: 79.365 %<br>Validation 3, accuracy: 85.714 %<br>Validation 4, accuracy: 77.419 %<br>Validation 5, accuracy: 74.194 %<br>Validation 6, accuracy: 82.258 %<br>Validation 7, accuracy: 80.645 %<br>Validation 8, accuracy: 79.032 %<br>Validation 9, accuracy: 82.258 %<br>Running spend 0.773550 s</p>
</blockquote>
<p><code>gla</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 68.182 %<br>Validation 1, accuracy: 68.182 %<br>Validation 2, accuracy: 72.727 %<br>Validation 3, accuracy: 57.143 %<br>Validation 4, accuracy: 61.905 %<br>Validation 5, accuracy: 76.190 %<br>Validation 6, accuracy: 76.190 %<br>Validation 7, accuracy: 66.667 %<br>Validation 8, accuracy: 71.429 %<br>Validation 9, accuracy: 76.190 %<br>Running spend 0.203716 s</p>
</blockquote>
<p><code>hay</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 68.750 %<br>Validation 1, accuracy: 75.000 %<br>Validation 2, accuracy: 75.000 %<br>Validation 3, accuracy: 62.500 %<br>Validation 4, accuracy: 62.500 %<br>Validation 5, accuracy: 62.500 %<br>Validation 6, accuracy: 62.500 %<br>Validation 7, accuracy: 87.500 %<br>Validation 8, accuracy: 68.750 %<br>Validation 9, accuracy: 66.667 %<br>Running spend 0.116895 s</p>
</blockquote>
<p><code>iri</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 93.333 %<br>Validation 2, accuracy: 93.333 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 93.333 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 93.333 %<br>Validation 9, accuracy: 92.857 %<br>Running spend 0.052544 s</p>
</blockquote>
<p><code>new</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 90.909 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 90.909 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 90.476 %<br>Running spend 0.121106 s</p>
</blockquote>
<p><code>win</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 90.909 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 90.909 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 90.476 %<br>Running spend 0.121106 s</p>
</blockquote>
<p><code>zoo</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 90.000 %<br>Validation 1, accuracy: 90.000 %<br>Validation 2, accuracy: 100.000 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 90.000 %<br>Validation 5, accuracy: 90.000 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 100.000 %<br>Validation 9, accuracy: 80.000 %<br>Running spend 0.070432 s</p>
</blockquote>
<h3 id="KD搜索树版本"><a href="#KD搜索树版本" class="headerlink" title="KD搜索树版本"></a>KD搜索树版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span>(i%num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_vector, val_vector, train_label, val_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_distance</span>(<span class="params">train_vector, test_vector</span>):</span><br><span class="line">    distance = []</span><br><span class="line">    count  = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">        count += (train_vector[i] - test_vector[i]) ** <span class="number">2</span></span><br><span class="line">    count = count ** <span class="number">0.5</span></span><br><span class="line">    distance.append(count)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KD_Tree</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, point, label, left=<span class="literal">None</span>, right=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.point = point</span><br><span class="line">        <span class="variable language_">self</span>.label = label</span><br><span class="line">        <span class="variable language_">self</span>.left = left</span><br><span class="line">        <span class="variable language_">self</span>.right = right</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, labels, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(vector) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    k = <span class="built_in">len</span>(vector[<span class="number">0</span>])</span><br><span class="line">    i = depth % k</span><br><span class="line">    sort = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(vector, labels), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>][i])</span><br><span class="line">    m_index = <span class="built_in">len</span>(sort) // <span class="number">2</span></span><br><span class="line">    m_point, m_label = sort[m_index]</span><br><span class="line">    l_points = [point <span class="keyword">for</span> point, _ <span class="keyword">in</span> sort[:m_index]]</span><br><span class="line">    l_labels = [label <span class="keyword">for</span> _, label <span class="keyword">in</span> sort[:m_index]]</span><br><span class="line">    r_points = [point <span class="keyword">for</span> point, _ <span class="keyword">in</span> sort[m_index + <span class="number">1</span>:]]</span><br><span class="line">    r_labels = [label <span class="keyword">for</span> _, label <span class="keyword">in</span> sort[m_index + <span class="number">1</span>:]]</span><br><span class="line">    l_node = build_tree(l_points, l_labels, depth + <span class="number">1</span>)</span><br><span class="line">    r_node = build_tree(r_points, r_labels, depth + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> KD_Tree(m_point, m_label, l_node, r_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">tree, test_vector, depth=<span class="number">0</span>, best=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line">    current_distance = cal_distance(tree.point, test_vector)</span><br><span class="line">    <span class="keyword">if</span> best <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> current_distance &lt; best[<span class="number">2</span>]:</span><br><span class="line">        best = (tree.point, tree.label, current_distance)</span><br><span class="line">    i = depth % <span class="built_in">len</span>(test_vector)</span><br><span class="line">    <span class="keyword">if</span> test_vector[i] &lt; tree.point[i]:</span><br><span class="line">        next_branch, other_branch= tree.left, tree.right</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_branch, other_branch= tree.right, tree.left</span><br><span class="line">    best = search_tree(next_branch, test_vector, depth + <span class="number">1</span>, best)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(test_vector[i] - tree.point[i]) &lt; best[<span class="number">2</span>]:</span><br><span class="line">        best = search_tree(other_branch, test_vector, depth + <span class="number">1</span>, best)</span><br><span class="line">    <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vector, label = read_excel(<span class="string">r&quot;E:\Document\MachineLearning\dataset\bal.xls&quot;</span>)</span><br><span class="line">start = time.perf_counter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">    kd_tree = build_tree(train_vector, train_label)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_label)):</span><br><span class="line">        best = search_tree(kd_tree, val_vector[j])</span><br><span class="line">        <span class="keyword">if</span> best[<span class="number">1</span>] == val_label[j]:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation %d, accuracy: %.3f %%&quot;</span>%(i, count / <span class="built_in">len</span>(val_vector)*<span class="number">100</span>))</span><br><span class="line">end = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running spend %f s&quot;</span>%(end - start))</span><br></pre></td></tr></table></figure>

<h4 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h4><p><code>bal</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 87.302 %<br>Validation 1, accuracy: 82.540 %<br>Validation 2, accuracy: 79.365 %<br>Validation 3, accuracy: 80.952 %<br>Validation 4, accuracy: 82.258 %<br>Validation 5, accuracy: 74.194 %<br>Validation 6, accuracy: 75.806 %<br>Validation 7, accuracy: 88.710 %<br>Validation 8, accuracy: 82.258 %<br>Validation 9, accuracy: 74.194 %<br>Running spend 0.082670 s</p>
</blockquote>
<p><code>gla</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 63.636 %<br>Validation 1, accuracy: 68.182 %<br>Validation 2, accuracy: 72.727 %<br>Validation 3, accuracy: 71.429 %<br>Validation 4, accuracy: 61.905 %<br>Validation 5, accuracy: 76.190 %<br>Validation 6, accuracy: 76.190 %<br>Validation 7, accuracy: 80.952 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 76.190 %<br>Running spend 0.087868 s</p>
</blockquote>
<p><code>hay</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 81.250 %<br>Validation 1, accuracy: 75.000 %<br>Validation 2, accuracy: 81.250 %<br>Validation 3, accuracy: 43.750 %<br>Validation 4, accuracy: 81.250 %<br>Validation 5, accuracy: 75.000 %<br>Validation 6, accuracy: 43.750 %<br>Validation 7, accuracy: 68.750 %<br>Validation 8, accuracy: 62.500 %<br>Validation 9, accuracy: 73.333 %<br>Running spend 0.021925 s</p>
</blockquote>
<p><code>iri</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 93.333 %<br>Validation 2, accuracy: 86.667 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 93.333 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 93.333 %<br>Validation 9, accuracy: 92.857 %<br>Running spend 0.021278 s</p>
</blockquote>
<p><code>new</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 95.455 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 95.238 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 85.714 %<br>Validation 9, accuracy: 80.952 %<br>Running spend 0.055435 s</p>
</blockquote>
<p><code>win</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 72.222 %<br>Validation 1, accuracy: 72.222 %<br>Validation 2, accuracy: 72.222 %<br>Validation 3, accuracy: 66.667 %<br>Validation 4, accuracy: 88.889 %<br>Validation 5, accuracy: 88.889 %<br>Validation 6, accuracy: 83.333 %<br>Validation 7, accuracy: 76.471 %<br>Validation 8, accuracy: 76.471 %<br>Validation 9, accuracy: 76.471 %<br>Running spend 0.200047 s</p>
</blockquote>
<p><code>zoo</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 90.000 %<br>Validation 1, accuracy: 90.000 %<br>Validation 2, accuracy: 100.000 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 90.000 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 100.000 %<br>Validation 9, accuracy: 100.000 %<br>Running spend 0.049036 s</p>
</blockquote>
<h3 id="分析与复盘"><a href="#分析与复盘" class="headerlink" title="分析与复盘"></a>分析与复盘</h3><p>在使用<code>bal</code>数据集时，使用KD树进行搜索节省了90%左右的时间成本，但不知道为啥在validation 1，3，7，8中使用有局限性的KD搜索识别准确率竟然高于全局搜索，挺神奇的。（之所以选择<code>bal</code>数据集是由于他的维度较少，数量较多，能更好发挥KD树的优势）</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/wow/" rel="tag">wow!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-241119"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/11/20/241119/"
    >24-11-19 log</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/11/20/241119/" class="article-date">
  <time datetime="2024-11-20T08:00:00.000Z" itemprop="datePublished">2024-11-20</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>昨天和今天勉勉强强在<em>ChatGPT</em>和一个<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ts4y1f7Gu/">B站视频</a>的帮助下写了一个静态的网站，然后部署到了<em>GitHub</em>上。虽然效果不尽人意，不知道为啥引用人家主题的时候在我本地运行的好好的，到了<em>GitHub</em>上就格式有问题。然而出人意料的是，到今天我写这个log文件的时候格式又好了，不知道是不是没有刷新导致的问题。<a href="https://cphoenixw.github.io/blog/">My Blog</a></p>
<p>我用的是一个 <em>Hexo</em> 开发框架，然后用了它 ayer 的一个主题。</p>
<p>下面是我的具体操作：（<strong>开发者模式</strong>）</p>
<p>0.调整 Git 配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;你的用户名&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;你的邮箱&quot;</span></span><br><span class="line"><span class="built_in">set</span> http_proxy=http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line"><span class="built_in">set</span> https_proxy=http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line">git config --global https.proxy http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line">git config --global https.proxy https://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br></pre></td></tr></table></figure>

<p>1.安装 <em>Hexo</em>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /d <span class="string">&quot;项目地址&quot;</span></span><br><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure>

<p>2.初始化项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init <span class="string">&quot;新项目名称&quot;</span></span><br><span class="line"><span class="built_in">cd</span> <span class="string">&quot;新项目名称&quot;</span></span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>

<p>3.在 <code>source/_posts</code> 文件夹中创建 Markdown 文件：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">Copy code</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line">title: 文章标题</span><br><span class="line">date: 2024-11-18</span><br><span class="line"><span class="section">tags:</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line">这里是正文内容。</span><br></pre></td></tr></table></figure>

<p>4.安装主题：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/Shen-Yu/hexo-theme-ayer.git themes/ayer</span><br></pre></td></tr></table></figure>

<p>5.GitHub 网页端配置</p>
<p>登录 GitHub。<br>点击右上角 + 按钮，选择 New Repository。<br>创建一个新仓库，例如 blog。</p>
<p>6.配置 ssh:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">&quot;你的邮箱&quot;</span></span><br></pre></td></tr></table></figure>

<p>按照 cmd 提示获取 ssh 密钥，用 edge 打开即可。</p>
<p>登录<em>GitHub</em>主页( <strong>不是项目主页</strong> )，进入 Settings &gt; SSH and GPG keys，点击 New SSH key，粘贴密钥；</p>
<p>在 Code &gt; SSH 中复制SSH地址</p>
<p>7.修改配置，根目录下的<code> _config.yml</code>里</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">ayer</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">    <span class="attr">repo:</span> <span class="string">&quot;ssh链接&quot;</span></span><br><span class="line">    <span class="attr">branch:</span> <span class="string">gh-pages</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>8.测试 SSH：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>

<p>返回值为Hi &lt;你的用户名&gt;!xxx就好了</p>
<p>9.部署:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>

<p>10.访问网站</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/happy/" rel="tag">happy!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-hello-world"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/11/18/hello-world/"
    >Hello World!</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/11/18/hello-world/" class="article-date">
  <time datetime="2024-11-18T14:02:59.000Z" itemprop="datePublished">2024-11-18</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>Welcome to PhoenixW’s blog! This is his first post.<br>刚刚在本地部署了一下博客，效果不错。<br>然而部署到github上的时候不知道为啥主题就加载不出来了。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/testing/" rel="tag">testing!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2024
        <i class="ri-heart-fill heart_icon"></i> Phoenix W
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/blog/"><img src="./images/ayer-side.svg" alt="Phoenix W"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/blog">主页</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/blog/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/blog/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/blog/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/blog/js/jquery-3.6.0.min.js"></script>
 
<script src="/blog/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/blog/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/blog/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->

<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>