<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Phoenix W</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="./favicon.ico" />
       
<link rel="stylesheet" href="/blog/dist/main.css">

      
<link rel="stylesheet" href="/blog/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/blog/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="./images/cover6.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/blog/">Phoenix W</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['What can I help with?', '这个人很懒，博客里啥都没写', ''],
        startDelay: 0,
        typeSpeed: 70,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-AdaBoost"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2025/01/20/AdaBoost/"
    >AdaBoost 算法（Adaptive Boosting）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2025/01/20/AdaBoost/" class="article-date">
  <time datetime="2025-01-19T16:00:00.000Z" itemprop="datePublished">2025-01-20</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>AdaBoost 算法（Adaptive Boosting）</h1>
<h2 id="1-数据输入">1.数据输入</h2>
<p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、测试集。</p>
<h2 id="2-模型训练">2.模型训练</h2>
<h3 id="数据准备">数据准备</h3>
<p>对每一个训练集进行有放回的10次随机采样。</p>
<p>初始化每组样本权重为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>为抽取样本数量</li>
</ul>
<h3 id="模型训练">模型训练</h3>
<h4 id="构建决策树">构建决策树</h4>
<p>直接调用 <code>decisionTreeCART.py</code> 中之前写的函数。</p>
<h4 id="寻找错误点">寻找错误点</h4>
<p>用训练集跑一遍刚刚构建的决策树模型。</p>
<h4 id="计算加权错误率">计算加权错误率</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\epsilon=\frac{\sum_{i=1}^N{w_i⋅\epsilon_i}}{\sum_{i=1}^N{w_i}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.841882em;vertical-align:-1.170941em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.670941em;"><span style="top:-2.128769em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6897100000000003em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.170941em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>$ \epsilon_i$  若为错误点取 1，若非错误点取 0</li>
<li>$ w_i $ 为样本权重</li>
<li>$ N $ 为样本总数</li>
</ul>
<h4 id="计算模型的权重">计算模型的权重</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⋅</mo><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>ϵ</mi><mo separator="true">,</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha=\frac{1}{2}⋅\ln({\frac{1-\epsilon}{\max(\epsilon,10^{-6})}})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.25744em;vertical-align:-0.936em;"></span><span class="mop">ln</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">max</span><span class="mopen">(</span><span class="mord mathdefault">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">6</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">ϵ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<h4 id="更新样本权重">更新样本权重</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>α</mi><mo>⋅</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">w_i=w_i⋅e^{-\alpha⋅\epsilon_i}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.59445em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.821331em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.0037em;">α</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<h4 id="归一化样本权重">归一化样本权重</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>w</mi><mi>i</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w_i = \frac{w_i}{\sum_{j=1}^{N}{w_j}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.414609em;vertical-align:-1.3070490000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.128769em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3070490000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h3 id="结果预测">结果预测</h3>
<h4 id="计算加权得分矩阵">计算加权得分矩阵</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>⋅</mo><msubsup><mi>p</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S_{i,c} = \sum_{t=1}^T{\alpha_t⋅p_t^{(c)}(x_i)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">c</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999997em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></p>
<ul>
<li>$ p_t^{©}(x_i) $ 为在第$$t$$个模型中，验证集中的第$$ i $$组特征对于标签$$c$$的得分</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> 为模型数量</li>
</ul>
<h4 id="判定预测结果">判定预测结果</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>y</mi><mi>i</mi></msub><mo>^</mo></mover><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>max</mi><mo>⁡</mo><msub><mi>S</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{y_i}=arg\max S_{i,c}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h2 id="3-代码复现">3.代码复现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> decisionTree.decisionTreeCART <span class="keyword">as</span> cart</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost 实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adaboost</span>(<span class="params">vector, label, n=<span class="number">10</span>, epsilon=<span class="number">0.9</span></span>):</span><br><span class="line">    n_samples = <span class="built_in">len</span>(label)</span><br><span class="line">    weights = np.ones(n_samples) / n_samples</span><br><span class="line">    trees = []</span><br><span class="line">    alphas = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 有权重的随机抽取</span></span><br><span class="line">        indices = np.random.choice(np.arange(n_samples), size=n_samples, p=weights)</span><br><span class="line">        sample_vector, sample_label = vector[indices], label[indices]</span><br><span class="line">        tree = cart.build_tree(sample_vector, sample_label, epsilon)</span><br><span class="line">        trees.append(tree)</span><br><span class="line">        predictions = np.array([cart.search_tree(tree, x) <span class="keyword">for</span> x <span class="keyword">in</span> vector])</span><br><span class="line">        <span class="comment"># 寻找错误点，转化为0，1</span></span><br><span class="line">        errors = (predictions != label).astype(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment"># 计算加权错误率</span></span><br><span class="line">        weighted_error = np.dot(weights, errors) / np.<span class="built_in">sum</span>(weights)</span><br><span class="line">        <span class="keyword">if</span> weighted_error &gt; <span class="number">0.5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        alpha = <span class="number">0.5</span> * np.log((<span class="number">1</span> - weighted_error) / <span class="built_in">max</span>(weighted_error, <span class="number">1e-6</span>))</span><br><span class="line">        alphas.append(alpha)</span><br><span class="line">        weights *= np.exp(-alpha * errors) / np.<span class="built_in">sum</span>(weights)</span><br><span class="line">    <span class="keyword">return</span> trees, alphas</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost 预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">trees, alphas, val_vector</span>):</span><br><span class="line">    c = <span class="built_in">len</span>(np.unique(label)) + <span class="number">1</span></span><br><span class="line">    n = <span class="built_in">len</span>(val_vector)</span><br><span class="line">    scores = np.zeros((n, c))</span><br><span class="line">    <span class="keyword">for</span> alpha, tree <span class="keyword">in</span> <span class="built_in">zip</span>(alphas,trees):</span><br><span class="line">        p = np.array([cart.search_tree(tree,value) <span class="keyword">for</span> value <span class="keyword">in</span> val_vector])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            scores[i, p[i]] += alpha</span><br><span class="line">    <span class="keyword">return</span> np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> item.is_file():</span><br><span class="line">            result.append(<span class="built_in">str</span>(item))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">        vector, label = cart.read_excel(<span class="built_in">dir</span>)</span><br><span class="line">        start = time.perf_counter()</span><br><span class="line">        count, <span class="built_in">all</span> = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            train_vector, train_label, val_vector, val_label = cart.division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">            trees, alphas = adaboost(train_vector, train_label, n=<span class="number">1</span>, epsilon=<span class="number">0.9</span>)</span><br><span class="line">            predictions = predict(trees, alphas, val_vector)</span><br><span class="line">            acc = np.mean(predictions == val_label)</span><br><span class="line">            count += acc * <span class="built_in">len</span>(val_label)</span><br><span class="line">            <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, accuracy: <span class="subst">&#123;acc * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">100</span> / <span class="built_in">all</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        end = time.perf_counter()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds\n&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-运行结果">4.运行结果</h2>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, epoch 1, accuracy: 85.000%<br>
Dataset <strong><code>bal</code></strong>, epoch 2, accuracy: 79.365%<br>
Dataset <strong><code>bal</code></strong>, epoch 3, accuracy: 73.016%<br>
Dataset <strong><code>bal</code></strong>, epoch 4, accuracy: 71.429%<br>
Dataset <strong><code>bal</code></strong>, epoch 5, accuracy: 74.603%<br>
Dataset <strong><code>bal</code></strong>, epoch 6, accuracy: 77.778%<br>
Dataset <strong><code>bal</code></strong>, epoch 7, accuracy: 87.302%<br>
Dataset <strong><code>bal</code></strong>, epoch 8, accuracy: 79.365%<br>
Dataset <strong><code>bal</code></strong>, epoch 9, accuracy: 77.778%<br>
Dataset <strong><code>bal</code></strong>, epoch 10, accuracy: 78.333%</p>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 78.365%<br>
Execution Time: 0.1762 seconds</p>
<p>Dataset <strong><code>gla</code></strong>, epoch 1, accuracy: 76.471%<br>
Dataset <strong><code>gla</code></strong>, epoch 2, accuracy: 69.565%<br>
Dataset <strong><code>gla</code></strong>, epoch 3, accuracy: 52.174%<br>
Dataset <strong><code>gla</code></strong>, epoch 4, accuracy: 78.261%<br>
Dataset <strong><code>gla</code></strong>, epoch 5, accuracy: 81.818%<br>
Dataset <strong><code>gla</code></strong>, epoch 6, accuracy: 50.000%<br>
Dataset <strong><code>gla</code></strong>, epoch 7, accuracy: 63.636%<br>
Dataset <strong><code>gla</code></strong>, epoch 8, accuracy: 52.381%<br>
Dataset <strong><code>gla</code></strong>, epoch 9, accuracy: 70.000%<br>
Dataset <strong><code>gla</code></strong>, epoch 10, accuracy: 70.000%</p>
<p>Dataset <strong><code>gla</code></strong>, average accuracy: 66.197%<br>
Execution Time: 0.9747 seconds</p>
<p>Dataset <strong><code>hay</code></strong>, epoch 1, accuracy: 93.333%<br>
Dataset <strong><code>hay</code></strong>, epoch 2, accuracy: 77.778%<br>
Dataset <strong><code>hay</code></strong>, epoch 3, accuracy: 82.353%<br>
Dataset <strong><code>hay</code></strong>, epoch 4, accuracy: 82.353%<br>
Dataset <strong><code>hay</code></strong>, epoch 5, accuracy: 82.353%<br>
Dataset <strong><code>hay</code></strong>, epoch 6, accuracy: 93.333%<br>
Dataset <strong><code>hay</code></strong>, epoch 7, accuracy: 73.333%<br>
Dataset <strong><code>hay</code></strong>, epoch 8, accuracy: 93.333%<br>
Dataset <strong><code>hay</code></strong>, epoch 9, accuracy: 86.667%<br>
Dataset <strong><code>hay</code></strong>, epoch 10, accuracy: 93.333%</p>
<p>Dataset <strong><code>hay</code></strong>, average accuracy: 85.535%<br>
Execution Time: 0.0445 seconds</p>
<p>Dataset <strong><code>iri</code></strong>, epoch 1, accuracy: 92.857%<br>
Dataset <strong><code>iri</code></strong>, epoch 2, accuracy: 93.333%<br>
Dataset <strong><code>iri</code></strong>, epoch 3, accuracy: 100.000%<br>
Dataset <strong><code>iri</code></strong>, epoch 4, accuracy: 86.667%<br>
Dataset <strong><code>iri</code></strong>, epoch 5, accuracy: 93.333%<br>
Dataset <strong><code>iri</code></strong>, epoch 6, accuracy: 93.333%<br>
Dataset <strong><code>iri</code></strong>, epoch 7, accuracy: 100.000%<br>
Dataset <strong><code>iri</code></strong>, epoch 8, accuracy: 86.667%<br>
Dataset <strong><code>iri</code></strong>, epoch 9, accuracy: 93.333%<br>
Dataset <strong><code>iri</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>iri</code></strong>, average accuracy: 93.960%<br>
Execution Time: 0.0763 seconds</p>
<p>Dataset <strong><code>new</code></strong>, epoch 1, accuracy: 90.000%<br>
Dataset <strong><code>new</code></strong>, epoch 2, accuracy: 86.364%<br>
Dataset <strong><code>new</code></strong>, epoch 3, accuracy: 90.909%<br>
Dataset <strong><code>new</code></strong>, epoch 4, accuracy: 81.818%<br>
Dataset <strong><code>new</code></strong>, epoch 5, accuracy: 100.000%<br>
Dataset <strong><code>new</code></strong>, epoch 6, accuracy: 95.455%<br>
Dataset <strong><code>new</code></strong>, epoch 7, accuracy: 90.476%<br>
Dataset <strong><code>new</code></strong>, epoch 8, accuracy: 100.000%<br>
Dataset <strong><code>new</code></strong>, epoch 9, accuracy: 100.000%<br>
Dataset <strong><code>new</code></strong>, epoch 10, accuracy: 95.238%</p>
<p>Dataset <strong><code>new</code></strong>, average accuracy: 92.991%<br>
Execution Time: 0.2705 seconds</p>
<p>Dataset <strong><code>win</code></strong>, epoch 1, accuracy: 100.000%<br>
Dataset <strong><code>win</code></strong>, epoch 2, accuracy: 94.737%<br>
Dataset <strong><code>win</code></strong>, epoch 3, accuracy: 94.444%<br>
Dataset <strong><code>win</code></strong>, epoch 4, accuracy: 77.778%<br>
Dataset <strong><code>win</code></strong>, epoch 5, accuracy: 100.000%<br>
Dataset <strong><code>win</code></strong>, epoch 6, accuracy: 88.889%<br>
Dataset <strong><code>win</code></strong>, epoch 7, accuracy: 88.889%<br>
Dataset <strong><code>win</code></strong>, epoch 8, accuracy: 94.444%<br>
Dataset <strong><code>win</code></strong>, epoch 9, accuracy: 83.333%<br>
Dataset <strong><code>win</code></strong>, epoch 10, accuracy: 87.500%</p>
<p>Dataset <strong><code>win</code></strong>, average accuracy: 90.960%<br>
Execution Time: 0.5513 seconds</p>
<p>Dataset <strong><code>zoo</code></strong>, epoch 1, accuracy: 100.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 2, accuracy: 91.667%<br>
Dataset <strong><code>zoo</code></strong>, epoch 3, accuracy: 91.667%<br>
Dataset <strong><code>zoo</code></strong>, epoch 4, accuracy: 100.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 5, accuracy: 90.909%<br>
Dataset <strong><code>zoo</code></strong>, epoch 6, accuracy: 90.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 7, accuracy: 100.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 8, accuracy: 100.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 9, accuracy: 100.000%<br>
Dataset <strong><code>zoo</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>
Execution Time: 0.0368 seconds</p>
</blockquote>
<h2 id="5-复盘与分析">5.复盘与分析</h2>
<p>这个模型的运行结果不像是其他模型，准确率会产生微小的变化。我怀疑可能是在调整权重值的时候由于是一些很小的浮点数计算，而产生了一些不确定性。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-Bagging"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2025/01/17/Bagging/"
    >Bagging 算法（Bootstrap Aggregating）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2025/01/17/Bagging/" class="article-date">
  <time datetime="2025-01-16T16:00:00.000Z" itemprop="datePublished">2025-01-17</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Bagging-算法（Bootstrap-Aggregating）"><a href="#Bagging-算法（Bootstrap-Aggregating）" class="headerlink" title="Bagging 算法（Bootstrap Aggregating）"></a>Bagging 算法（Bootstrap Aggregating）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、测试集。</p>
<h2 id="2-随机采样"><a href="#2-随机采样" class="headerlink" title="2.随机采样"></a>2.随机采样</h2><p>对每一个训练集进行有放回的10次随机采样</p>
<h2 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3.模型训练"></a>3.模型训练</h2><p>五次用于训练决策树模型，五次用于训练逻辑斯蒂回归模型。</p>
<p>直接调用<code>decisionTreeCART.py</code> 和 <code>LogisticRegression.py</code> 中之前写的函数。</p>
<h2 id="4-模型集成"><a href="#4-模型集成" class="headerlink" title="4.模型集成"></a>4.模型集成</h2><p>用所有模型对验证集进行，通过多数投票法决定出最后的预测结果。</p>
<h2 id="5-代码复现"><a href="#5-代码复现" class="headerlink" title="5.代码复现"></a>5.代码复现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> decisionTree.decisionTreeCART <span class="keyword">as</span> cart</span><br><span class="line"><span class="keyword">from</span> LogisticRegression.LogisticRegression <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集按标签随机抽取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bootstrap_sample</span>(<span class="params">vector, label</span>):</span><br><span class="line">    n_samples = vector.shape[<span class="number">0</span>]</span><br><span class="line">    indices = np.random.choice(n_samples, n_samples, replace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> vector[indices], label[indices]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bagging模型实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bagging</span>(<span class="params">vector, label, n_trees=<span class="number">5</span>, n_logistic=<span class="number">5</span>, epsilon=<span class="number">0.01</span></span>):</span><br><span class="line">    trees = []</span><br><span class="line">    logistic_models = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_trees):</span><br><span class="line">        train_vector, train_label = bootstrap_sample(vector, label)</span><br><span class="line">        tree = cart.build_tree(train_vector, train_label, epsilon)</span><br><span class="line">        trees.append(tree)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_logistic):</span><br><span class="line">        train_vector, train_label = bootstrap_sample(vector, label)</span><br><span class="line">        logistic_model = LogisticRegression(learning_rate=<span class="number">0.01</span>, epochs=<span class="number">50000</span>)</span><br><span class="line">        logistic_model.fit(train_vector, train_label)</span><br><span class="line">        logistic_models.append(logistic_model)</span><br><span class="line">    <span class="keyword">return</span> trees, logistic_models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">trees, logistic_models, value</span>):</span><br><span class="line">    tree_predictions = [cart.search_tree(tree, value) <span class="keyword">for</span> tree <span class="keyword">in</span> trees]</span><br><span class="line">    logistic_predictions = [logistic_model.predict(value.reshape(<span class="number">1</span>, -<span class="number">1</span>))[<span class="number">0</span>] <span class="keyword">for</span> logistic_model <span class="keyword">in</span> logistic_models]</span><br><span class="line">    all_predictions = tree_predictions + logistic_predictions</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(<span class="built_in">set</span>(all_predictions), key=all_predictions.count)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> item.is_file():</span><br><span class="line">            result.append(<span class="built_in">str</span>(item))</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">        vector, label = cart.read_excel(<span class="built_in">dir</span>)</span><br><span class="line">        start = time.perf_counter()</span><br><span class="line">        count, <span class="built_in">all</span> = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            train_vector, train_label, val_vector, val_label = cart.division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">            trees, logistic_models = bagging(train_vector, train_label, n_trees=<span class="number">5</span>, n_logistic=<span class="number">5</span>, epsilon=<span class="number">0.9</span>)</span><br><span class="line">            pre_count = count</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">                <span class="keyword">if</span> predict(trees, logistic_models, val_vector[j]) == val_label[j]:</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">            <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">            acc = (count - pre_count) / <span class="built_in">len</span>(val_label)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, accuracy: <span class="subst">&#123;acc * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">100</span> / <span class="built_in">all</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        end = time.perf_counter()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds\n&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="6-运行结果"><a href="#6-运行结果" class="headerlink" title="6.运行结果"></a>6.运行结果</h2><blockquote>
<p>Dataset <strong><code>bal</code></strong>, epoch 1, accuracy: 90.000%<br>Dataset <strong><code>bal</code></strong>, epoch 2, accuracy: 90.476%<br>Dataset <strong><code>bal</code></strong>, epoch 3, accuracy: 88.889%<br>Dataset <strong><code>bal</code></strong>, epoch 4, accuracy: 85.714%<br>Dataset <strong><code>bal</code></strong>, epoch 5, accuracy: 90.476%<br>Dataset <strong><code>bal</code></strong>, epoch 6, accuracy: 87.302%<br>Dataset <strong><code>bal</code></strong>, epoch 7, accuracy: 88.889%<br>Dataset <strong><code>bal</code></strong>, epoch 8, accuracy: 85.714%<br>Dataset <strong><code>bal</code></strong>, epoch 9, accuracy: 90.476%<br>Dataset <strong><code>bal</code></strong>, epoch 10, accuracy: 91.667%</p>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 88.942%<br>Execution Time: 133.4536 seconds</p>
<p>Dataset <strong><code>gla</code></strong>, epoch 1, accuracy: 70.588%<br>Dataset <strong><code>gla</code></strong>, epoch 2, accuracy: 69.565%<br>Dataset <strong><code>gla</code></strong>, epoch 3, accuracy: 69.565%<br>Dataset <strong><code>gla</code></strong>, epoch 4, accuracy: 78.261%<br>Dataset <strong><code>gla</code></strong>, epoch 5, accuracy: 81.818%<br>Dataset <strong><code>gla</code></strong>, epoch 6, accuracy: 54.545%<br>Dataset <strong><code>gla</code></strong>, epoch 7, accuracy: 68.182%<br>Dataset <strong><code>gla</code></strong>, epoch 8, accuracy: 52.381%<br>Dataset <strong><code>gla</code></strong>, epoch 9, accuracy: 80.000%<br>Dataset <strong><code>gla</code></strong>, epoch 10, accuracy: 75.000%</p>
<p>Dataset <strong><code>gla</code></strong>, average accuracy: 69.953%<br>Execution Time: 72.9844 seconds</p>
<p>Dataset <strong><code>hay</code></strong>, epoch 1, accuracy: 80.000%<br>Dataset <strong><code>hay</code></strong>, epoch 2, accuracy: 77.778%<br>Dataset <strong><code>hay</code></strong>, epoch 3, accuracy: 70.588%<br>Dataset <strong><code>hay</code></strong>, epoch 4, accuracy: 82.353%<br>Dataset <strong><code>hay</code></strong>, epoch 5, accuracy: 82.353%<br>Dataset <strong><code>hay</code></strong>, epoch 6, accuracy: 73.333%<br>Dataset <strong><code>hay</code></strong>, epoch 7, accuracy: 53.333%<br>Dataset <strong><code>hay</code></strong>, epoch 8, accuracy: 86.667%<br>Dataset <strong><code>hay</code></strong>, epoch 9, accuracy: 66.667%<br>Dataset <strong><code>hay</code></strong>, epoch 10, accuracy: 53.333%</p>
<p>Dataset <strong><code>hay</code></strong>, average accuracy: 72.956%<br>Execution Time: 26.9276 seconds</p>
<p>Dataset <strong><code>iri</code></strong>, epoch 1, accuracy: 92.857%<br>Dataset <strong><code>iri</code></strong>, epoch 2, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 4, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, epoch 5, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>iri</code></strong>, average accuracy: 97.315%<br>Execution Time: 25.3448 seconds</p>
<p>Dataset <strong><code>new</code></strong>, epoch 1, accuracy: 80.000%<br>Dataset <strong><code>new</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 4, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 6, accuracy: 95.455%<br>Dataset <strong><code>new</code></strong>, epoch 7, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 9, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>new</code></strong>, average accuracy: 96.729%<br>Execution Time: 49.1436 seconds</p>
<p>Dataset <strong><code>win</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 2, accuracy: 89.474%<br>Dataset <strong><code>win</code></strong>, epoch 3, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 4, accuracy: 83.333%<br>Dataset <strong><code>win</code></strong>, epoch 5, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 6, accuracy: 83.333%<br>Dataset <strong><code>win</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 8, accuracy: 88.889%<br>Dataset <strong><code>win</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>win</code></strong>, average accuracy: 93.220%<br>Execution Time: 62.5098 seconds</p>
<p>Dataset <strong><code>zoo</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 3, accuracy: 91.667%<br>Dataset <strong><code>zoo</code></strong>, epoch 4, accuracy: 91.667%<br>Dataset <strong><code>zoo</code></strong>, epoch 5, accuracy: 90.909%<br>Dataset <strong><code>zoo</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 7, accuracy: 88.889%<br>Dataset <strong><code>zoo</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 87.3832 seconds</p>
</blockquote>
<h2 id="7-复盘与分析"><a href="#7-复盘与分析" class="headerlink" title="7.复盘与分析"></a>7.复盘与分析</h2><p><code>Bagging</code> 方法是一种 <code>集成学习</code> 技术，对原先代码的封装性和可调用行比较高，体现在调用逻辑斯蒂回归模型时一个函数就能搞定，而调用决策树模型时还对原来的代码做了一些适应性调整。这个方法整体上没啥算法难度。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第五周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2025/01/11/%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第五周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2025/01/11/%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2025-01-10T16:00:00.000Z" itemprop="datePublished">2025-01-11</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集和测试集。</p>
<h2 id="2-模型构建"><a href="#2-模型构建" class="headerlink" title="2.模型构建"></a>2.模型构建</h2><h3 id="初始化网络参数"><a href="#初始化网络参数" class="headerlink" title="初始化网络参数"></a>初始化网络参数</h3><ul>
<li><p>输入层：大小为输入特征的维度数。</p>
</li>
<li><p>隐藏层：这个模型只有一层隐藏层，大小我设为定值<code>30</code>。</p>
</li>
<li><p>输出层：大小为标签种类数。</p>
</li>
</ul>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>通过上一轮训练的参数计算各个输出层的结果。</p>
<h5 id="输入层到隐藏层线性变换："><a href="#输入层到隐藏层线性变换：" class="headerlink" title="输入层到隐藏层线性变换："></a>输入层到隐藏层线性变换：</h5><p>$$<br>z_1&#x3D;XW_1+b_1<br>$$</p>
<ul>
<li>输入向量 X</li>
<li>输入层到隐藏层的权重 W<sub>1</sub></li>
<li>输入层到隐藏层偏置 b<sub>1</sub></li>
</ul>
<h5 id="隐藏层激活运算："><a href="#隐藏层激活运算：" class="headerlink" title="隐藏层激活运算："></a>隐藏层激活运算：</h5><p>$$<br>a _1&#x3D;ReLU(z_1)&#x3D;max(0,z_1)<br>$$</p>
<h5 id="隐藏层到输出层的线性变换："><a href="#隐藏层到输出层的线性变换：" class="headerlink" title="隐藏层到输出层的线性变换："></a>隐藏层到输出层的线性变换：</h5><p>$$<br>z_2&#x3D;a_1W_2+b_2<br>$$</p>
<ul>
<li>隐藏层到输出层的权重W<sub>2</sub></li>
<li>隐藏层到输出层的偏置 b<sub>2</sub></li>
</ul>
<h5 id="返回计算结果："><a href="#返回计算结果：" class="headerlink" title="返回计算结果："></a>返回计算结果：</h5><p>$$<br>a_2&#x3D;z_2<br>$$</p>
<h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>用逻辑斯蒂回归中同样的交叉熵损失函数算出损失值。然而实际运算时只用进行<code>softmax</code>而不用算损失值，因为后面要对损失值求偏导数，这个过程可以被化简。</p>
<h5 id="通过-Softmax-函数计算输出层的概率："><a href="#通过-Softmax-函数计算输出层的概率：" class="headerlink" title="通过 Softmax 函数计算输出层的概率："></a>通过 <code>Softmax</code> 函数计算输出层的概率：</h5><p>$$<br>p_{i,c}&#x3D;\frac{e^{a_{i,c}}}{\sum_{j&#x3D;1}^C{e^{a_{i,j}}}}<br>$$</p>
<ul>
<li>p<sub>i,c </sub>是指样本 i 对应类别 c 的预测结果</li>
<li>C 是总类别数</li>
<li>a<sub>i,c</sub> 是</li>
</ul>
<h5 id="通过交叉熵损失函数算出损失值："><a href="#通过交叉熵损失函数算出损失值：" class="headerlink" title="通过交叉熵损失函数算出损失值："></a>通过交叉熵损失函数算出损失值：</h5><p>$$<br>L&#x3D;-\frac{1}{m}\sum_{i&#x3D;1}^m\sum_{c&#x3D;1}^C y_{i,c}⋅\ln(p_{i,c})<br>$$</p>
<ul>
<li><p>m 是样本总数</p>
</li>
<li><p>C 是类别总数</p>
</li>
<li><p>y<sub>i,c</sub> 是指样本 i 对应类别 c 的真实标签</p>
<p>满足如果样本 i 属于类别 c，那么 y<sub>i,c</sub> &#x3D; 1，否则 y<sub>i,c</sub> &#x3D; 0</p>
</li>
</ul>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><h5 id="输出层参数梯度计算："><a href="#输出层参数梯度计算：" class="headerlink" title="输出层参数梯度计算："></a>输出层参数梯度计算：</h5><ul>
<li>基础公式</li>
</ul>
<p>$$<br>\frac{∂L}{∂z_2}&#x3D; p-y<br>$$</p>
<ul>
<li>W<sub>2 </sub>的梯度</li>
</ul>
<p>$$<br>\frac{∂L}{∂W_2}&#x3D; \frac{∂L}{∂z_2}⋅\frac{∂{z_2}}{∂W_2}&#x3D;(p−y)⋅a_1^T<br>$$</p>
<ul>
<li>b<sub>2</sub> 的梯度</li>
</ul>
<p>$$<br>\frac{∂L}{∂b_2}&#x3D; \frac{∂L}{∂z_2}⋅\frac{∂{z_2}}{∂b_2}&#x3D;p−y<br>$$</p>
<h5 id="隐藏层参数梯度计算："><a href="#隐藏层参数梯度计算：" class="headerlink" title="隐藏层参数梯度计算："></a>隐藏层参数梯度计算：</h5><ul>
<li>基础公式</li>
</ul>
<p>$$<br>\frac{∂L}{∂z_1}&#x3D;\frac{∂L}{∂a_1}⋅\frac{∂{a_1}}{∂z_1}&#x3D;\frac{∂L}{∂z_2}⋅\frac{∂{z_2}}{∂a_1}⋅\frac{∂{a_1}}{∂z_1}&#x3D;(p−y)⋅W_2^T⋅ReLu’(z_1)<br>$$</p>
<ul>
<li><p>W<sub>1 </sub>的梯度<br>$$<br>\frac{∂L}{∂W_1}&#x3D;\frac{∂L}{∂z_1}⋅\frac{∂{z_1}}{∂W_1}&#x3D;(p−y)⋅W_2^T⋅ReLu’(z_1)⋅X^T<br>$$</p>
</li>
<li><p>b<sub>1 </sub>的梯度<br>$$<br>\frac{∂L}{∂b_1}&#x3D;\frac{∂L}{∂z_1}⋅\frac{∂{z_1}}{∂b_1}&#x3D;(p−y)⋅W_2^T⋅ReLu’(z_1)<br>$$</p>
</li>
</ul>
<h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p>$$<br>W_1&#x3D;W_1−η\frac{∂L}{∂W_1}<br>$$</p>
<p>$$<br>b_1&#x3D;b_1−η\frac{∂L}{∂b_1}<br>$$</p>
<p>$$<br>W_2&#x3D;W_2−η\frac{∂L}{∂W_2}<br>$$</p>
<p>$$<br>b_2&#x3D;b_2−η\frac{∂L}{∂b_2}<br>$$</p>
<ul>
<li>η 为学习率</li>
</ul>
<h3 id="模型运行"><a href="#模型运行" class="headerlink" title="模型运行"></a>模型运行</h3><p><img src="https://github.com/CPhoenixW/blog/blob/gh-pages/images/5-1.jpg?raw=true" alt="5-1"></p>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据读取函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values  <span class="comment"># 特征向量</span></span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values  <span class="comment"># 标签</span></span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">vector</span>):</span><br><span class="line">    integer_mask = np.<span class="built_in">all</span>(vector == vector.astype(<span class="built_in">int</span>), axis=<span class="number">0</span>)</span><br><span class="line">    float_mask = ~integer_mask</span><br><span class="line">    <span class="comment"># 独热编码离散特征</span></span><br><span class="line">    integer_features = vector[:, integer_mask]</span><br><span class="line">    <span class="keyword">if</span> integer_features.size &gt; <span class="number">0</span>:</span><br><span class="line">        encoder = OneHotEncoder(sparse_output=<span class="literal">False</span>)</span><br><span class="line">        integer_features_encoded = encoder.fit_transform(integer_features)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        integer_features_encoded = np.empty((vector.shape[<span class="number">0</span>], <span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 标准化和归一化连续特征</span></span><br><span class="line">    float_features = vector[:, float_mask]</span><br><span class="line">    <span class="keyword">if</span> float_features.size &gt; <span class="number">0</span>:</span><br><span class="line">        means = np.mean(float_features, axis=<span class="number">0</span>)</span><br><span class="line">        stds = np.std(float_features, axis=<span class="number">0</span>)</span><br><span class="line">        stds[stds == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        standardized_features = (float_features - means) / stds</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        standardized_features = np.empty((vector.shape[<span class="number">0</span>], <span class="number">0</span>))</span><br><span class="line">    processed_vector = np.hstack([integer_features_encoded, standardized_features])</span><br><span class="line">    <span class="keyword">return</span> processed_vector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector, train_label = [], []</span><br><span class="line">    val_vector, val_label = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> i % num != c:</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(val_vector), np.array(train_label), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, learning_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.input_size = input_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.output_size = output_size</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="comment"># 输入层参数与偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.W1 = np.random.randn(<span class="variable language_">self</span>.input_size, <span class="variable language_">self</span>.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.b1 = np.zeros((<span class="number">1</span>, <span class="variable language_">self</span>.hidden_size))</span><br><span class="line">        <span class="comment"># 隐藏层参数与偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.W2 = np.random.randn(<span class="variable language_">self</span>.hidden_size, <span class="variable language_">self</span>.output_size)</span><br><span class="line">        <span class="variable language_">self</span>.b2 = np.zeros((<span class="number">1</span>, <span class="variable language_">self</span>.output_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">self, x</span>):</span><br><span class="line">        exp_x = np.exp(x - np.<span class="built_in">max</span>(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">        <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="variable language_">self</span>.z1 = np.dot(X, <span class="variable language_">self</span>.W1) + <span class="variable language_">self</span>.b1</span><br><span class="line">        <span class="comment"># 隐藏层输出 a1</span></span><br><span class="line">        <span class="variable language_">self</span>.a1 = np.maximum(<span class="number">0</span>, <span class="variable language_">self</span>.z1)</span><br><span class="line">        <span class="variable language_">self</span>.z2 = np.dot(<span class="variable language_">self</span>.a1, <span class="variable language_">self</span>.W2) + <span class="variable language_">self</span>.b2</span><br><span class="line">        <span class="comment"># 输出层输出 a2</span></span><br><span class="line">        <span class="variable language_">self</span>.a2 = <span class="variable language_">self</span>.z2</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.z2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, X, y, p</span>):</span><br><span class="line">        <span class="comment"># 两个基础梯度</span></span><br><span class="line">        m = y.shape[<span class="number">0</span>]</span><br><span class="line">        grad_z2 = <span class="variable language_">self</span>.softmax(p)</span><br><span class="line">        grad_z2[<span class="built_in">range</span>(m), y] -= <span class="number">1</span></span><br><span class="line">        grad_z2 /= m </span><br><span class="line">        grad_z1 = np.dot(grad_z2, <span class="variable language_">self</span>.W2.T) * np.where(<span class="variable language_">self</span>.z1 &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        <span class="variable language_">self</span>.W2 -= <span class="variable language_">self</span>.learning_rate * np.dot(<span class="variable language_">self</span>.a1.T, grad_z2)</span><br><span class="line">        <span class="variable language_">self</span>.b2 -= <span class="variable language_">self</span>.learning_rate * np.<span class="built_in">sum</span>(grad_z2, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W1 -= <span class="variable language_">self</span>.learning_rate * np.dot(X.T, grad_z1)</span><br><span class="line">        <span class="variable language_">self</span>.b1 -= <span class="variable language_">self</span>.learning_rate * np.<span class="built_in">sum</span>(grad_z1, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs=<span class="number">10000</span></span>):</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            p = <span class="variable language_">self</span>.forward(X)</span><br><span class="line">            <span class="variable language_">self</span>.backward(X, y, p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        y_pred = <span class="variable language_">self</span>.forward(X)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> item.is_file():</span><br><span class="line">            dataset_name = item.stem</span><br><span class="line">            vector, label = read_excel(item)</span><br><span class="line">            vector = preprocess_data(vector)</span><br><span class="line">            <span class="comment"># 确定类别数量</span></span><br><span class="line">            num_classes = <span class="built_in">len</span>(np.unique(label))</span><br><span class="line">            <span class="comment"># 转换标签为整数</span></span><br><span class="line">            label = label.astype(<span class="built_in">int</span>) - <span class="number">1</span></span><br><span class="line">            start = time.perf_counter()</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="built_in">all</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">                train_vector, val_vector, train_label, val_label = dataset_division(vector, label, num=<span class="number">10</span>, c=i)</span><br><span class="line">                <span class="comment"># 初始化神经网络</span></span><br><span class="line">                input_size = train_vector.shape[<span class="number">1</span>]</span><br><span class="line">                hidden_size = <span class="number">30</span>  <span class="comment"># 可以调整</span></span><br><span class="line">                model = NeuralNetwork(input_size, hidden_size, num_classes, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">                model.train(train_vector, train_label, epochs=<span class="number">50000</span>)</span><br><span class="line">                predictions = model.predict(val_vector)</span><br><span class="line">                count += np.<span class="built_in">sum</span>(predictions == val_label)</span><br><span class="line">                <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">                acc = np.<span class="built_in">sum</span>(predictions == val_label) / <span class="built_in">len</span>(val_label)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Dataset <span class="subst">&#123;dataset_name&#125;</span>, fold <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, accuracy: <span class="subst">&#123;acc * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            accuracy = count / <span class="built_in">all</span></span><br><span class="line">            end = time.perf_counter()</span><br><span class="line">            execution_time = end - start</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Dataset <span class="subst">&#123;dataset_name&#125;</span>, average accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;execution_time:<span class="number">.4</span>f&#125;</span> seconds\n&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="4-运行结果"><a href="#4-运行结果" class="headerlink" title="4.运行结果"></a>4.运行结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch = <span class="number">100000</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">hidden_size = <span class="number">30</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>Dataset <strong><code>bal</code></strong>, fold 1, accuracy: 96.825%<br>Dataset <strong><code>bal</code></strong>, fold 2, accuracy: 92.063%<br>Dataset <strong><code>bal</code></strong>, fold 3, accuracy: 90.476%<br>Dataset <strong><code>bal</code></strong>, fold 4, accuracy: 95.238%<br>Dataset <strong><code>bal</code></strong>, fold 5, accuracy: 87.097%<br>Dataset <strong><code>bal</code></strong>, fold 6, accuracy: 91.935%<br>Dataset <strong><code>bal</code></strong>, fold 7, accuracy: 95.161%<br>Dataset <strong><code>bal</code></strong>, fold 8, accuracy: 96.774%<br>Dataset <strong><code>bal</code></strong>, fold 9, accuracy: 93.548%<br>Dataset <strong><code>bal</code></strong>, fold 10, accuracy: 93.548%</p>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 93.269%<br>Execution Time: 1190.4770 seconds</p>
<p>Dataset <strong><code>gla</code></strong>, fold 1, accuracy: 68.182%<br>Dataset <strong><code>gla</code></strong>, fold 2, accuracy: 63.636%<br>Dataset <strong><code>gla</code></strong>, fold 3, accuracy: 72.727%<br>Dataset <strong><code>gla</code></strong>, fold 4, accuracy: 76.190%<br>Dataset <strong><code>gla</code></strong>, fold 5, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, fold 6, accuracy: 80.952%<br>Dataset <strong><code>gla</code></strong>, fold 7, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, fold 8, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, fold 9, accuracy: 66.667%<br>Dataset <strong><code>gla</code></strong>, fold 10, accuracy: 71.429%</p>
<p>Dataset <strong><code>gla</code></strong>, average accuracy: 71.362%<br>Execution Time: 251.4495 seconds</p>
<p>Dataset <strong><code>hay</code></strong>, fold 1, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, fold 2, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, fold 3, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, fold 4, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, fold 5, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, fold 6, accuracy: 62.500%<br>Dataset <strong><code>hay</code></strong>, fold 7, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, fold 8, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, fold 9, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, fold 10, accuracy: 73.333%</p>
<p>Dataset <strong><code>hay</code></strong>, average accuracy: 81.761%<br>Execution Time: 240.0420 seconds</p>
<p>Dataset <strong><code>iri</code></strong>, fold 1, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, fold 2, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, fold 3, accuracy: 86.667%<br>Dataset <strong><code>iri</code></strong>, fold 4, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, fold 5, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, fold 6, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, fold 7, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, fold 8, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, fold 9, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, fold 10, accuracy: 92.857%</p>
<p>Dataset <strong><code>iri</code></strong>, average accuracy: 95.302%<br>Execution Time: 213.1071 seconds</p>
<p>Dataset <strong><code>new</code></strong>, fold 1, accuracy: 81.818%<br>Dataset <strong><code>new</code></strong>, fold 2, accuracy: 90.909%<br>Dataset <strong><code>new</code></strong>, fold 3, accuracy: 95.455%<br>Dataset <strong><code>new</code></strong>, fold 4, accuracy: 95.455%<br>Dataset <strong><code>new</code></strong>, fold 5, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, fold 6, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, fold 7, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, fold 8, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, fold 9, accuracy: 85.714%<br>Dataset <strong><code>new</code></strong>, fold 10, accuracy: 85.714%</p>
<p>Dataset <strong><code>new</code></strong>, average accuracy: 92.523%<br>Execution Time: 960.9057 seconds</p>
<p>Dataset <strong><code>win</code></strong>, fold 1, accuracy: 83.333%<br>Dataset <strong><code>win</code></strong>, fold 2, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, fold 3, accuracy: 88.889%<br>Dataset <strong><code>win</code></strong>, fold 4, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, fold 5, accuracy: 83.333%<br>Dataset <strong><code>win</code></strong>, fold 6, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, fold 7, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, fold 8, accuracy: 82.353%<br>Dataset <strong><code>win</code></strong>, fold 9, accuracy: 94.118%<br>Dataset <strong><code>win</code></strong>, fold 10, accuracy: 94.118%</p>
<p>Dataset <strong><code>win</code></strong>, average accuracy: 91.525%<br>Execution Time: 791.6697 seconds</p>
<p>Dataset <strong><code>zoo</code></strong>, fold 1, accuracy: 90.000%<br>Dataset <strong><code>zoo</code></strong>, fold 2, accuracy: 70.000%<br>Dataset <strong><code>zoo</code></strong>, fold 3, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 4, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 5, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 6, accuracy: 70.000%<br>Dataset <strong><code>zoo</code></strong>, fold 7, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 8, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 9, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, fold 10, accuracy: 90.000%</p>
<p>Dataset <strong><code>zoo</code></strong>, average accuracy: 92.000%<br>Execution Time: 85.7256 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><p>不知道GPU对训练时间的优化体现下了哪里，我用 <code>Pytorch</code> 复现了一遍原本代码，发现训练的时间更长了，而且对GPU的利用率超级低，只有0%，连内存都只用了0.2%，可能是我的模型太过于简单了，只有一个隐藏层导致的？</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第四周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/28/%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第四周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/28/%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-27T16:00:00.000Z" itemprop="datePublished">2024-12-28</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="逻辑斯蒂回归分类（Logistic-Regression）"><a href="#逻辑斯蒂回归分类（Logistic-Regression）" class="headerlink" title="逻辑斯蒂回归分类（Logistic Regression）"></a>逻辑斯蒂回归分类（Logistic Regression）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>对于某个样本 i，y<sub>k</sub> &#x3D; 1 如果该样本属于类别 k，否则 y<sub>k</sub> &#x3D; 0。</p>
<p>y^k 是模型预测的类别 k 的概率，通过<code>Softmax</code>函数计算得出。</p>
<p>K是类别数。<br>$$<br>L&#x3D;-\sum_{k&#x3D;1}^Ky_k⋅\ln(\hat{y_k})<br>$$<br>Z<sub>i,k</sub>&#x3D;X<sub>i</sub>W<sub>k</sub>+b<sub>k</sub> 是每个样本 i 的类别 k 的逻辑斯蒂加权和。</p>
<p>X<sub>i</sub>是样本 i 的输入特征，W<sub>k</sub>是类别 k 的权重，b<sub>k</sub>是类别 k 的偏置。<br>$$<br>\hat{y_k}&#x3D;\sum_{i&#x3D;1}^N{\frac{e^{Z_{i,k}}}{\sum_{j&#x3D;1}^Ke^{Z_{i,j}}}}<br>$$</p>
<p>$$<br>Z_{i,k}&#x3D;X_iW_k+b_k<br>$$</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><h4 id="通过将L对W求偏导数得出W的梯度："><a href="#通过将L对W求偏导数得出W的梯度：" class="headerlink" title="通过将L对W求偏导数得出W的梯度："></a>通过将L对W求偏导数得出W的梯度：</h4><p>$$<br>\frac{∂L_i}{∂W_k}&#x3D;\frac{∂L_i}{∂Z_{i,k}}⋅\frac{∂Z_{i,k}}{∂W_k}<br>$$</p>
<h5 id="第一项有："><a href="#第一项有：" class="headerlink" title="第一项有："></a>第一项有：</h5><p>$$<br>\frac{∂L_i}{∂Z_{i,k}}&#x3D;\frac{∂L_i}{∂\hat{y_{i,m}}}⋅\frac{∂\hat{y_{i,m}}}{∂Z_{i,k}}&#x3D;\sum_{m&#x3D;1}^K(-\frac{y_{i,m}}{\hat{y_{i,m}}})⋅\hat{y_{i,m}}(δ_{k.m}-\hat{y_{i,m})}&#x3D;\hat{y_{i,k}}-y_{i,k}<br>$$</p>
<p>δ<sub>k,m</sub>代表 k&#x3D;m 时取 1，否则为 0。</p>
<h5 id="第二项有："><a href="#第二项有：" class="headerlink" title="第二项有："></a>第二项有：</h5><p>$$<br>\frac{∂Z_{i,k}}{∂W_k}&#x3D;X_i<br>$$</p>
<h5 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h5><p>$$<br>\frac{∂L}{∂W_k}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(\hat{y_{i,k}}-y_{i,k})⋅X_i<br>$$</p>
<p>$$<br>\frac{∂L}{∂W}&#x3D;\frac{1}{N}X^T(\hat{y}-y)<br>$$</p>
<h4 id="通过将L对b求偏导数得出偏置b的梯度："><a href="#通过将L对b求偏导数得出偏置b的梯度：" class="headerlink" title="通过将L对b求偏导数得出偏置b的梯度："></a>通过将L对b求偏导数得出偏置b的梯度：</h4><h5 id="同理可得："><a href="#同理可得：" class="headerlink" title="同理可得："></a>同理可得：</h5><p>$$<br>\frac{∂L}{∂b}&#x3D;\frac{1}{N}(\hat{y}-y)<br>$$</p>
<h3 id="梯度下降："><a href="#梯度下降：" class="headerlink" title="梯度下降："></a>梯度下降：</h3><p>每次计算时，将现有权重和偏置减去梯度值，不断梯度下降。</p>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取Excel数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">vector</span>):</span><br><span class="line">    continuous = []</span><br><span class="line">    discrete = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(vector.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> (vector[:, i] == vector[:, i].astype(<span class="built_in">int</span>)).<span class="built_in">all</span>():</span><br><span class="line">            discrete.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            continuous.append(i)</span><br><span class="line">    <span class="comment"># 离散特征独热化</span></span><br><span class="line">    <span class="keyword">if</span> discrete:</span><br><span class="line">        one_hot_encoder = OneHotEncoder(sparse_output=<span class="literal">False</span>)</span><br><span class="line">        one_hot_encoded = one_hot_encoder.fit_transform(vector[:, discrete])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        one_hot_encoded = np.empty((vector.shape[<span class="number">0</span>], <span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 连续特征规则化</span></span><br><span class="line">    <span class="keyword">if</span> continuous:</span><br><span class="line">        preprocess = vector[:, continuous]</span><br><span class="line">        means = np.mean(preprocess, axis=<span class="number">0</span>)</span><br><span class="line">        stds = np.std(preprocess, axis=<span class="number">0</span>)</span><br><span class="line">        stds[stds == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        standardized = (preprocess - means) / stds</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        standardized = np.empty((vector.shape[<span class="number">0</span>], <span class="number">0</span>))</span><br><span class="line">    processed_vector = np.hstack((one_hot_encoded, standardized))</span><br><span class="line">    <span class="keyword">return</span> processed_vector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector, train_label = [], []</span><br><span class="line">    val_vector, val_label = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> (i % num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(val_vector), np.array(train_label), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="variable language_">self</span>.epochs = epochs</span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义softmax函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_softmax</span>(<span class="params">self, z</span>):</span><br><span class="line">        exp_z = np.exp(z - np.<span class="built_in">max</span>(z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))  <span class="comment"># 防止溢出</span></span><br><span class="line">        <span class="keyword">return</span> exp_z / np.<span class="built_in">sum</span>(exp_z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义one_hot将标签转化为向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_one_hot</span>(<span class="params">self, label, num_classes</span>):</span><br><span class="line">        one_hot = np.zeros((label.shape[<span class="number">0</span>], num_classes))</span><br><span class="line">        one_hot[np.arange(label.shape[<span class="number">0</span>]), label] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型拟合函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, vector, label</span>):</span><br><span class="line">        num_samples, num_features = vector.shape</span><br><span class="line">        num_classes = np.<span class="built_in">max</span>(label) + <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(num_features, num_classes) * <span class="number">0.01</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(num_classes)</span><br><span class="line">        y_one_hot = <span class="variable language_">self</span>._one_hot(label, num_classes)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        weight = np.zeros((num_features, num_classes))</span><br><span class="line">        <span class="comment"># 梯度下降循环</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.epochs):</span><br><span class="line">            <span class="keyword">if</span> weight.<span class="built_in">all</span>() == <span class="variable language_">self</span>.weights.<span class="built_in">all</span>():</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> count &gt; <span class="variable language_">self</span>.epochs // <span class="number">3</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                weight = <span class="variable language_">self</span>.weights</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">            logits = np.dot(vector, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">            y_pred = <span class="variable language_">self</span>._softmax(logits)</span><br><span class="line">            gradient_w = np.dot(vector.T, (y_pred - y_one_hot)) / num_samples</span><br><span class="line">            gradient_b = np.<span class="built_in">sum</span>(y_pred - y_one_hot, axis=<span class="number">0</span>) / num_samples</span><br><span class="line">            <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learning_rate * gradient_w</span><br><span class="line">            <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learning_rate * gradient_b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测效果</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, vector</span>):</span><br><span class="line">        logits = np.dot(vector, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        y_pred = <span class="variable language_">self</span>._softmax(logits)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = LogisticRegression(learning_rate=<span class="number">0.1</span>, epochs=<span class="number">10000</span>)</span><br><span class="line">    base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> item.is_file():</span><br><span class="line">            dataset_name = item.stem</span><br><span class="line">            vector, label = read_excel(item)</span><br><span class="line">            vector = preprocess_data(vector)</span><br><span class="line">            start = time.perf_counter()</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="built_in">all</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">                train_vector, val_vector, train_label, val_label = dataset_division(vector, label, num=<span class="number">10</span>, c=i)</span><br><span class="line">                model.fit(train_vector, train_label)</span><br><span class="line">                predictions = model.predict(val_vector)</span><br><span class="line">                count += np.<span class="built_in">sum</span>(predictions == val_label)</span><br><span class="line">                <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">                acc = np.<span class="built_in">sum</span>(predictions == val_label) / <span class="built_in">len</span>(val_label)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;dataset_name&#125;</span>`**, epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, accuracy: <span class="subst">&#123;acc * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            accuracy = count / <span class="built_in">all</span></span><br><span class="line">            end = time.perf_counter()</span><br><span class="line">            execution_time = end - start</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;dataset_name&#125;</span>`**, average accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;execution_time:<span class="number">.4</span>f&#125;</span> seconds\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, epoch 1, accuracy: 98.413%<br>Dataset <strong><code>bal</code></strong>, epoch 2, accuracy: 96.825%<br>Dataset <strong><code>bal</code></strong>, epoch 3, accuracy: 87.302%<br>Dataset <strong><code>bal</code></strong>, epoch 4, accuracy: 98.413%<br>Dataset <strong><code>bal</code></strong>, epoch 5, accuracy: 90.323%<br>Dataset <strong><code>bal</code></strong>, epoch 6, accuracy: 91.935%<br>Dataset <strong><code>bal</code></strong>, epoch 7, accuracy: 95.161%<br>Dataset <strong><code>bal</code></strong>, epoch 8, accuracy: 96.774%<br>Dataset <strong><code>bal</code></strong>, epoch 9, accuracy: 93.548%<br>Dataset <strong><code>bal</code></strong>, epoch 10, accuracy: 98.387%</p>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 94.712%<br>Execution Time: 12.0552 seconds</p>
<p>Dataset <strong><code>gla</code></strong>, epoch 1, accuracy: 72.727%<br>Dataset <strong><code>gla</code></strong>, epoch 2, accuracy: 63.636%<br>Dataset <strong><code>gla</code></strong>, epoch 3, accuracy: 63.636%<br>Dataset <strong><code>gla</code></strong>, epoch 4, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 5, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 6, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 7, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, epoch 8, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, epoch 9, accuracy: 76.190%<br>Dataset <strong><code>gla</code></strong>, epoch 10, accuracy: 61.905%</p>
<p>Dataset <strong><code>gla</code></strong>, average accuracy: 65.258%<br>Execution Time: 6.9824 seconds</p>
<p>Dataset <strong><code>hay</code></strong>, epoch 1, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 2, accuracy: 75.000%<br>Dataset <strong><code>hay</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>hay</code></strong>, epoch 4, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, epoch 5, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 6, accuracy: 62.500%<br>Dataset <strong><code>hay</code></strong>, epoch 7, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 8, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, epoch 9, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 10, accuracy: 80.000%</p>
<p>Dataset <strong><code>hay</code></strong>, average accuracy: 81.761%<br>Execution Time: 5.5748 seconds</p>
<p>Dataset <strong><code>iri</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 3, accuracy: 86.667%<br>Dataset <strong><code>iri</code></strong>, epoch 4, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 10, accuracy: 92.857%</p>
<p>Dataset <strong><code>iri</code></strong>, average accuracy: 97.987%<br>Execution Time: 5.1322 seconds</p>
<p>Dataset <strong><code>new</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 2, accuracy: 95.455%<br>Dataset <strong><code>new</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 4, accuracy: 90.909%<br>Dataset <strong><code>new</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 7, accuracy: 90.476%<br>Dataset <strong><code>new</code></strong>, epoch 8, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, epoch 9, accuracy: 90.476%<br>Dataset <strong><code>new</code></strong>, epoch 10, accuracy: 90.476%</p>
<p>Dataset <strong><code>new</code></strong>, average accuracy: 95.327%<br>Execution Time: 7.7811 seconds</p>
<p>Dataset <strong><code>win</code></strong>, epoch 1, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 3, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 4, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 5, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 6, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 8, accuracy: 82.353%<br>Dataset <strong><code>win</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>win</code></strong>, average accuracy: 95.480%<br>Execution Time: 10.4406 seconds</p>
<p>Dataset <strong><code>zoo</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 2, accuracy: 80.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 4, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 6, accuracy: 90.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 10, accuracy: 90.000%</p>
<p>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 5.7426 seconds</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">epoach = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Dataset <strong><code>bal</code></strong>, epoch 1, accuracy: 98.413%<br>Dataset <strong><code>bal</code></strong>, epoch 2, accuracy: 95.238%<br>Dataset <strong><code>bal</code></strong>, epoch 3, accuracy: 85.714%<br>Dataset <strong><code>bal</code></strong>, epoch 4, accuracy: 96.825%<br>Dataset <strong><code>bal</code></strong>, epoch 5, accuracy: 88.710%<br>Dataset <strong><code>bal</code></strong>, epoch 6, accuracy: 87.097%<br>Dataset <strong><code>bal</code></strong>, epoch 7, accuracy: 95.161%<br>Dataset <strong><code>bal</code></strong>, epoch 8, accuracy: 95.161%<br>Dataset <strong><code>bal</code></strong>, epoch 9, accuracy: 93.548%<br>Dataset <strong><code>bal</code></strong>, epoch 10, accuracy: 95.161%</p>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 93.109%<br>Execution Time: 12.6704 seconds</p>
<p>Dataset <strong><code>gla</code></strong>, epoch 1, accuracy: 68.182%<br>Dataset <strong><code>gla</code></strong>, epoch 2, accuracy: 59.091%<br>Dataset <strong><code>gla</code></strong>, epoch 3, accuracy: 68.182%<br>Dataset <strong><code>gla</code></strong>, epoch 4, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 5, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 6, accuracy: 57.143%<br>Dataset <strong><code>gla</code></strong>, epoch 7, accuracy: 76.190%<br>Dataset <strong><code>gla</code></strong>, epoch 8, accuracy: 71.429%<br>Dataset <strong><code>gla</code></strong>, epoch 9, accuracy: 76.190%<br>Dataset <strong><code>gla</code></strong>, epoch 10, accuracy: 57.143%</p>
<p>Dataset <strong><code>gla</code></strong>, average accuracy: 64.789%<br>Execution Time: 7.1636 seconds</p>
<p>Dataset <strong><code>hay</code></strong>, epoch 1, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 2, accuracy: 75.000%<br>Dataset <strong><code>hay</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>hay</code></strong>, epoch 4, accuracy: 87.500%<br>Dataset <strong><code>hay</code></strong>, epoch 5, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 6, accuracy: 62.500%<br>Dataset <strong><code>hay</code></strong>, epoch 7, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 8, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 9, accuracy: 81.250%<br>Dataset <strong><code>hay</code></strong>, epoch 10, accuracy: 73.333%</p>
<p>Dataset <strong><code>hay</code></strong>, average accuracy: 80.503%<br>Execution Time: 5.8169 seconds</p>
<p>Dataset <strong><code>iri</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 3, accuracy: 86.667%<br>Dataset <strong><code>iri</code></strong>, epoch 4, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 7, accuracy: 93.333%<br>Dataset <strong><code>iri</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>iri</code></strong>, epoch 10, accuracy: 92.857%</p>
<p>Dataset <strong><code>iri</code></strong>, average accuracy: 97.315%<br>Execution Time: 5.2833 seconds</p>
<p>Dataset <strong><code>new</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 2, accuracy: 95.455%<br>Dataset <strong><code>new</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 4, accuracy: 90.909%<br>Dataset <strong><code>new</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 6, accuracy: 100.000%<br>Dataset <strong><code>new</code></strong>, epoch 7, accuracy: 90.476%<br>Dataset <strong><code>new</code></strong>, epoch 8, accuracy: 95.238%<br>Dataset <strong><code>new</code></strong>, epoch 9, accuracy: 90.476%<br>Dataset <strong><code>new</code></strong>, epoch 10, accuracy: 90.476%</p>
<p>Dataset <strong><code>new</code></strong>, average accuracy: 95.327%<br>Execution Time: 7.8368 seconds</p>
<p>Dataset <strong><code>win</code></strong>, epoch 1, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 2, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 3, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 4, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 5, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 6, accuracy: 94.444%<br>Dataset <strong><code>win</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 8, accuracy: 82.353%<br>Dataset <strong><code>win</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>win</code></strong>, epoch 10, accuracy: 100.000%</p>
<p>Dataset <strong><code>win</code></strong>, average accuracy: 95.480%<br>Execution Time: 10.4644 seconds</p>
<p>Dataset <strong><code>zoo</code></strong>, epoch 1, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 2, accuracy: 80.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 3, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 4, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 5, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 6, accuracy: 90.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 7, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 8, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 9, accuracy: 100.000%<br>Dataset <strong><code>zoo</code></strong>, epoch 10, accuracy: 90.000%</p>
<p>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.000%<br>Execution Time: 5.4742 seconds</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.05</span></span><br><span class="line">epoach = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><p>感觉<code>LearningRate</code>和<code>epoach</code>对准确度的影响好像没那么大。</p>
<p>当模型参数长时间不再优化时提前结束可以大大提高效率，并且对总体影响不大。</p>
<p><img src="https://raw.githubusercontent.com/CPhoenixW/blog/refs/heads/gh-pages/images/4-1.png" alt="4-1"></p>
<p>区分连续变量和离散变量对整体运行结果有很大提升。</p>
<p><img src="https://github.com/CPhoenixW/blog/blob/gh-pages/images/4-2.png?raw=true" alt="4-2"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第三周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/21/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第三周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/21/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-20T16:00:00.000Z" itemprop="datePublished">2024-12-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="朴素贝叶斯算法（Naive-Bayes）"><a href="#朴素贝叶斯算法（Naive-Bayes）" class="headerlink" title="朴素贝叶斯算法（Naive Bayes）"></a>朴素贝叶斯算法（Naive Bayes）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、验证集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="计算先验概率"><a href="#计算先验概率" class="headerlink" title="计算先验概率"></a>计算先验概率</h3><p>$$<br>P(C_K)&#x3D;\frac{|C_k|}{N}<br>$$</p>
<p>P(C<sub>k</sub>)为标签属于某一类别的概率。</p>
<h3 id="计算特征的似然值"><a href="#计算特征的似然值" class="headerlink" title="计算特征的似然值"></a>计算特征的似然值</h3><p>C<sub>k</sub>表示某一个标签，x表示某一个特征，d表示所取样本数量，P(x<sub>i</sub>|C<sub>k</sub>)表示某特征的每个特征值在C<sub>k</sub>下的概率。<br>$$<br>P(x|C_k)&#x3D;\prod_{i&#x3D;1}^d{P(x_i|C_k)}<br>$$<br>由于数据集中存在较多连续变量，不妨假设每个标签下的各个特征值满足正态分布。<code>sigma</code>代表代表标准差，<code>u</code>代表平均值。<br>$$<br>P(x_i|C_k)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-u)^2}{2\sigma^2}}<br>$$<br>P(x|C<sub>k</sub>)为每个特征的似然值。</p>
<h3 id="计算后验概率"><a href="#计算后验概率" class="headerlink" title="计算后验概率"></a>计算后验概率</h3><p>根据贝叶斯定理：<br>$$<br>P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}<br>$$<br>得出后验概率计算公式：</p>
<p>$$<br>P(C_k|x)&#x3D;\frac{P(C_k)P(x|C_k)}{P(A)}\propto P(C_k)P(x|C_k)<br>$$<br>带入之前计算的先验概率和似然值，选择后验概率最大的作为分类。<br>$$<br>\hat{C}&#x3D;arg\underset{C_k}{max}P(C_k|x)<br>$$</p>
<blockquote>
<p>⚠️⚠️⚠️</p>
<p>训练集负责计算模型参数：先验概率P(C<sub>k</sub>)、均值<code>u</code>、方差<code>sigma</code>。</p>
<p>测试机负责评估模型参数：利用模型参数计算似然值与后验概率。</p>
</blockquote>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取Excel数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> (i % num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(val_vector), np.array(train_label), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianNaiveBayes</span>:</span><br><span class="line">    <span class="comment"># 通过训练集构建模型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, vector, label</span>):</span><br><span class="line">        <span class="variable language_">self</span>.classes = np.unique(label)</span><br><span class="line">        <span class="variable language_">self</span>.preprob = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.means = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">vars</span> = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> values <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">            divided = vector[label == values]</span><br><span class="line">            <span class="variable language_">self</span>.preprob[values] = <span class="built_in">len</span>(divided) / <span class="built_in">len</span>(label)</span><br><span class="line">            <span class="variable language_">self</span>.means[values] = np.mean(divided, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">vars</span>[values] = np.var(divided, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">vars</span>[values][<span class="variable language_">self</span>.<span class="built_in">vars</span>[values] == <span class="number">0</span>] = <span class="number">1e-9</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测测试集分类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, labels</span>):</span><br><span class="line">        predictions = []</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            postprob = []</span><br><span class="line">            <span class="keyword">for</span> value <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">                prior = np.log(<span class="variable language_">self</span>.preprob[value])</span><br><span class="line">                likelihoods = <span class="variable language_">self</span>.cal_likelihood(label, value)</span><br><span class="line">                likelihoods[likelihoods == <span class="number">0</span>] = <span class="number">1e-9</span></span><br><span class="line">                likelihood = np.<span class="built_in">sum</span>(np.log(likelihoods))</span><br><span class="line">                postprob.append(prior + likelihood)</span><br><span class="line">            predictions.append(<span class="variable language_">self</span>.classes[np.argmax(postprob)])</span><br><span class="line">        <span class="keyword">return</span> np.array(predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算似然值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_likelihood</span>(<span class="params">self, value, label</span>):</span><br><span class="line">        mean = <span class="variable language_">self</span>.means[label]</span><br><span class="line">        var = <span class="variable language_">self</span>.<span class="built_in">vars</span>[label]</span><br><span class="line">        var = np.where(var == <span class="number">0</span>, <span class="number">1e-9</span>, var)</span><br><span class="line">        <span class="comment"># 正态分布公式</span></span><br><span class="line">        likelihood = (<span class="number">1</span> / np.sqrt(<span class="number">2</span> * np.pi * var)) * np.exp(-<span class="number">0.5</span> * ((value - mean) ** <span class="number">2</span>) / var) </span><br><span class="line">        likelihood[likelihood &lt; <span class="number">1e-9</span>] = <span class="number">1e-9</span></span><br><span class="line">        <span class="keyword">return</span> likelihood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = GaussianNaiveBayes()</span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        dataset_name = item.stem</span><br><span class="line">        vector, label = read_excel(item)</span><br><span class="line">        start = time.perf_counter()</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="built_in">all</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">            model.fit(train_vector, train_label)</span><br><span class="line">            predictions = model.predict(val_vector)</span><br><span class="line">            count += np.<span class="built_in">sum</span>(predictions == val_label)</span><br><span class="line">            <span class="built_in">all</span> += <span class="built_in">len</span>(val_label)</span><br><span class="line">        accuracy = count / <span class="built_in">all</span></span><br><span class="line">        end = time.perf_counter()</span><br><span class="line">        execution_time = end - start</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;dataset_name&#125;</span>`**, average accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;execution_time:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 90.865%<br>Execution Time: 0.0236 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 46.948%<br>Execution Time: 0.0143 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 67.296%<br>Execution Time: 0.0062 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 95.302%<br>Execution Time: 0.0058 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 96.729%<br>Execution Time: 0.0080 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 98.305%<br>Execution Time: 0.0066 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 91.000%<br>Execution Time: 0.0093 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><p>在计算后验概率和似然值的时候选择了对每一组概率取对数，避免因数值过小而产生浮点数误差。</p>
<p><img src="https://github.com/CPhoenixW/blog/blob/gh-pages/images/2-1.png?raw=true" alt="2-1"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第二周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/07/%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第二周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/07/%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-12-06T16:00:00.000Z" itemprop="datePublished">2024-12-07</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集、验证集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="决策标准"><a href="#决策标准" class="headerlink" title="决策标准"></a>决策标准</h3><p>三种决策标准:</p>
<h4 id="信息增益值"><a href="#信息增益值" class="headerlink" title="信息增益值"></a>信息增益值</h4><p>D为全体样本，D<sub>i</sub>为某一特征下的各个取值，C<sub>k</sub>为各种标签，A为某一特征。由此可知n为特征下样本的取值个数，K为标签类别数。<br>$$<br>H(D) &#x3D; -\sum_{k&#x3D;1}^K \frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}<br>$$</p>
<p>$$<br>H(D|A)&#x3D;-\sum_{i&#x3D;1}^n \frac{|D_i|}{|D|}\sum_{k&#x3D;1}^K \frac{|D_{ik}|}{|D_i|}\log_2 \frac{|D_{ik}|}{|D_i|}<br>$$</p>
<p>$$<br>g(D,A)&#x3D;H(D)-H(D|A)<br>$$</p>
<p>得出信息增益值 g(D,A)</p>
<h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>适用于特征下取值较多时，用来避免高信息增益而低价值的判断。</p>
<p>$$<br>H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}<br>$$</p>
<p>$$<br>g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)}<br>$$</p>
<p>得出信息增益比 g<sub>R</sub>(D,A)</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>t某一特征，N<sub>i</sub>代表这个特征下不同标签类别，k为特征的总数。<br>$$<br>p_i&#x3D;\frac{|N_i|}{|N|}<br>$$</p>
<p>$$<br>Gini(t)&#x3D;1-\sum_{i&#x3D;1}^k p_i^2<br>$$</p>
<p>得出基尼指数 Gini(t)</p>
<h4 id="基尼分裂指数"><a href="#基尼分裂指数" class="headerlink" title="基尼分裂指数"></a>基尼分裂指数</h4><p>N<sub>L</sub>为待分裂的左子树，N<sub>R</sub>为待分裂的右子树。<br>$$<br>Gini_{split}&#x3D;\frac{N_L}{N_L+N_R}Gini(L)+\frac{N_L}{N_L+N_R}Gini(R)<br>$$<br>得出基尼分裂指数 Gini<sub>split</sub></p>
<h3 id="生成算法"><a href="#生成算法" class="headerlink" title="生成算法"></a>生成算法</h3><p>三种决策树生成算法：</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>以信息增益值为标准，选择信息增益值最大的维度为根节点，递归。</p>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>以信息增益比为标准，选择信息增益比最大的维度为根节点，递归。</p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>先根据选择基尼指数最小的作为根节点，再通过基尼分裂指数最小的选择分类方式，递归。</p>
<h3 id="剪枝算法"><a href="#剪枝算法" class="headerlink" title="剪枝算法"></a>剪枝算法</h3><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>当构建决策树时决策信息增益比过小时或基尼指数过大时，直接进行预剪枝，即把子树用用最多的标签代替。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>决策树构建完成后，自下而上进行扫描。若剪枝之后验证集的损失更小，就进行剪枝操作。</p>
<h2 id="3-代码复现"><a href="#3-代码复现" class="headerlink" title="3.代码复现"></a>3.代码复现</h2><h3 id="C4-5算法实现"><a href="#C4-5算法实现" class="headerlink" title="C4.5算法实现"></a>C4.5算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> i % num != c:</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(train_label), np.array(val_vector), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算熵值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_entropy</span>(<span class="params">label</span>):</span><br><span class="line">    a, counts = np.unique(label, return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(label)</span><br><span class="line">    entropy = -np.<span class="built_in">sum</span>(frac * np.log2(frac))</span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算信息增益比</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gain_ratio</span>(<span class="params">vector, label, i</span>):</span><br><span class="line">    <span class="comment"># 计算整体样本的熵值</span></span><br><span class="line">    entropy = cal_entropy(label)</span><br><span class="line">    di, counts = np.unique(vector[:, i], return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(vector)</span><br><span class="line">    gain = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(di)):</span><br><span class="line">        son = label[vector[:, i] == di[j]]</span><br><span class="line">        <span class="comment"># 计算某类标签的熵值</span></span><br><span class="line">        gain -= frac[j] * cal_entropy(son)</span><br><span class="line">    <span class="comment"># 计算固有值</span></span><br><span class="line">    IV = -np.<span class="built_in">sum</span>(frac * np.log2(frac))</span><br><span class="line">    <span class="keyword">if</span> IV == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    ratio = (entropy - gain) / IV</span><br><span class="line">    <span class="keyword">return</span> ratio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, label, epsilon=<span class="number">0.01</span>, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(label)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: label[<span class="number">0</span>]&#125;</span><br><span class="line">    <span class="keyword">if</span> vector.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    max_ratio, max_id = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vector[<span class="number">0</span>])):</span><br><span class="line">        ratio = cal_gain_ratio(vector, label, i)</span><br><span class="line">        <span class="keyword">if</span> ratio &gt; max_ratio:</span><br><span class="line">            max_ratio = ratio</span><br><span class="line">            max_id = i</span><br><span class="line">    <span class="comment"># 最大增益比小于阈值直接返回最多的类</span></span><br><span class="line">    <span class="keyword">if</span> max_ratio &lt; epsilon:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    values = np.unique(vector[:, max_id])</span><br><span class="line">    decision_tree = &#123;<span class="string">&#x27;point&#x27;</span>: max_id, <span class="string">&#x27;son&#x27;</span>: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> values:</span><br><span class="line">        son_vector = vector[vector[:, max_id] == value]</span><br><span class="line">        son_label = label[vector[:, max_id] == value]</span><br><span class="line">        son_vector = np.delete(son_vector, max_id, axis=<span class="number">1</span>)</span><br><span class="line">        decision_tree[<span class="string">&#x27;son&#x27;</span>][value] = build_tree(son_vector, son_label, epsilon, depth + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">decision_tree, value</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        point = decision_tree[<span class="string">&#x27;point&#x27;</span>]</span><br><span class="line">        feature_value = value[point]</span><br><span class="line">        <span class="keyword">if</span> feature_value <span class="keyword">in</span> decision_tree[<span class="string">&#x27;son&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][feature_value], value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.bincount(value.astype(<span class="built_in">int</span>)).argmax()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树剪枝</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cut_tree</span>(<span class="params">decision_tree, test_vector, test_label</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree</span><br><span class="line">    <span class="keyword">for</span> value, subtree <span class="keyword">in</span> decision_tree[<span class="string">&#x27;son&#x27;</span>].items():</span><br><span class="line">        decision_tree[<span class="string">&#x27;son&#x27;</span>][value] = cut_tree(subtree, test_vector, test_label)</span><br><span class="line">    post_label = np.bincount(test_label).argmax()</span><br><span class="line">    pre_error = cal_error(decision_tree, test_vector, test_label)</span><br><span class="line">    post_tree = &#123;<span class="string">&#x27;label&#x27;</span>: post_label&#125;</span><br><span class="line">    post_error = cal_error(post_tree, test_vector, test_label)</span><br><span class="line">    <span class="keyword">if</span> post_error &lt;= pre_error:</span><br><span class="line">        <span class="keyword">return</span> post_tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用测试集计算误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_error</span>(<span class="params">decision_tree, test_vector, test_label</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">        <span class="keyword">if</span> search_tree(decision_tree, test_vector[i]) != test_label[i]:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count / <span class="built_in">len</span>(test_vector)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        result.append(<span class="built_in">str</span>(item))</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">    vector, label = read_excel(<span class="built_in">dir</span>)</span><br><span class="line">    start = time.perf_counter()</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train_vector, train_label, test_vector, test_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">        train_vector, train_label, val_vector, val_label = dataset_division(train_vector, train_label, <span class="number">10</span>, i)</span><br><span class="line">        tree = build_tree(train_vector, train_label, epsilon=<span class="number">0.1</span>)</span><br><span class="line">        tree = cut_tree(tree, test_vector, test_label)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">            <span class="keyword">if</span> search_tree(tree, val_vector[j]) == val_label[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">10</span> / <span class="built_in">len</span>(val_label):<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    end = time.perf_counter()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 54.286%<br>Execution Time: 0.1993 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 34.211%<br>Execution Time: 0.3535 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 47.857%<br>Execution Time: 0.0382 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 33.846%<br>Execution Time: 0.0674 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 70.000%<br>Execution Time: 0.1220 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 39.375%<br>Execution Time: 0.4322 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 37.778%<br>Execution Time: 0.1743 seconds</p>
</blockquote>
<h3 id="CART算法复现"><a href="#CART算法复现" class="headerlink" title="CART算法复现"></a>CART算法复现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> i % num != c:</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line">    <span class="keyword">return</span> np.array(train_vector), np.array(train_label), np.array(val_vector), np.array(val_label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算基尼指数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gini</span>(<span class="params">label</span>):</span><br><span class="line">    _, counts = np.unique(label, return_counts=<span class="literal">True</span>)</span><br><span class="line">    frac = counts / <span class="built_in">len</span>(label)</span><br><span class="line">    gini = <span class="number">1</span> - np.<span class="built_in">sum</span>(frac ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> gini</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算最好的数据拆分点</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_gini_index</span>(<span class="params">vector, label, i</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    best_gini = <span class="number">1000</span></span><br><span class="line">    best_split = <span class="literal">None</span></span><br><span class="line">    best_mask_left = <span class="literal">None</span></span><br><span class="line">    <span class="built_in">sorted</span> = np.sort(np.unique(vector[:, i]))</span><br><span class="line">    splits = (<span class="built_in">sorted</span>[:-<span class="number">1</span>] + <span class="built_in">sorted</span>[<span class="number">1</span>:]) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> splits:</span><br><span class="line">        left_mask, right_mask = vector[:, i] &lt;= split, vector[:, i] &gt; split</span><br><span class="line">        left_labels, right_labels = label[left_mask], label[right_mask]</span><br><span class="line">        left_gini, right_gini = cal_gini(left_labels), cal_gini(right_labels)</span><br><span class="line">        gini_index = (<span class="built_in">len</span>(left_labels) / n) * left_gini + (<span class="built_in">len</span>(right_labels) / n) * right_gini</span><br><span class="line">        <span class="keyword">if</span> gini_index &lt; best_gini:</span><br><span class="line">            best_gini = gini_index</span><br><span class="line">            best_split = split</span><br><span class="line">            best_mask_left = left_mask</span><br><span class="line">    <span class="keyword">if</span> best_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> best_gini, best_split, best_mask_left</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_gini, best_split, best_mask_left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, label, epsilon=<span class="number">0.01</span>, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(label)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: label[<span class="number">0</span>]&#125;</span><br><span class="line">    <span class="keyword">if</span> vector.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    min_gini_index = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">    best_feature = <span class="literal">None</span></span><br><span class="line">    best_split = <span class="literal">None</span></span><br><span class="line">    best_mask_left = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vector[<span class="number">0</span>])):</span><br><span class="line">        gini_index, split, mask_left = cal_gini_index(vector, label, i)</span><br><span class="line">        <span class="keyword">if</span> gini_index &lt; min_gini_index:</span><br><span class="line">            min_gini_index = gini_index</span><br><span class="line">            best_feature = i</span><br><span class="line">            best_split = split</span><br><span class="line">            best_mask_left = mask_left</span><br><span class="line">    <span class="comment"># 最佳拆分点基尼指数过大，直接用最多的标签替代</span></span><br><span class="line">    <span class="keyword">if</span> min_gini_index &gt;= epsilon:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.bincount(label).argmax()&#125;</span><br><span class="line">    left_vector = vector[best_mask_left]</span><br><span class="line">    left_label = label[best_mask_left]</span><br><span class="line">    right_vector = vector[~best_mask_left]</span><br><span class="line">    right_label = label[~best_mask_left]</span><br><span class="line">    decision_tree = &#123;</span><br><span class="line">        <span class="string">&#x27;point&#x27;</span>: best_feature,</span><br><span class="line">        <span class="string">&#x27;split&#x27;</span>: best_split,</span><br><span class="line">        <span class="string">&#x27;son&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;left&#x27;</span>: build_tree(left_vector, left_label, epsilon, depth + <span class="number">1</span>),</span><br><span class="line">            <span class="string">&#x27;right&#x27;</span>: build_tree(right_vector, right_label, epsilon, depth + <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> decision_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">decision_tree, value</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> decision_tree:</span><br><span class="line">        <span class="keyword">return</span> decision_tree[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        point = decision_tree[<span class="string">&#x27;point&#x27;</span>]</span><br><span class="line">        feature_value = value[point]</span><br><span class="line">        <span class="keyword">if</span> feature_value &lt;= decision_tree[<span class="string">&#x27;split&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][<span class="string">&#x27;left&#x27;</span>], value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> search_tree(decision_tree[<span class="string">&#x27;son&#x27;</span>][<span class="string">&#x27;right&#x27;</span>], value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_path = Path(<span class="string">r&quot;E:\Document\MachineLearning\dataset&quot;</span>)</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> base_path.rglob(<span class="string">&#x27;*.xls&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> item.is_file():</span><br><span class="line">        result.append(<span class="built_in">str</span>(item))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> result:</span><br><span class="line">    vector, label = read_excel(<span class="built_in">dir</span>)</span><br><span class="line">    start = time.perf_counter()</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train_vector, train_label, test_vector, test_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">        train_vector, train_label, val_vector, val_label = dataset_division(train_vector, train_label, <span class="number">10</span>, i)</span><br><span class="line">        tree = build_tree(train_vector, train_label, epsilon=<span class="number">0.9</span>)</span><br><span class="line">        <span class="comment"># 神奇了，不剪枝效果竟然更好</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">            <span class="keyword">if</span> search_tree(tree, val_vector[j]) == val_label[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset **`<span class="subst">&#123;<span class="built_in">dir</span>[-<span class="number">7</span>:-<span class="number">4</span>]&#125;</span>`**, average accuracy: <span class="subst">&#123;count * <span class="number">10</span> / <span class="built_in">len</span>(val_label):<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    end = time.perf_counter()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Execution Time: <span class="subst">&#123;end - start:<span class="number">.4</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong>(未后剪枝)</p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 77.857%<br>Execution Time: 0.1870 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 71.053%<br>Execution Time: 1.4938 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 84.286%<br>Execution Time: 0.0395 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 96.154%<br>Execution Time: 0.0880 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 94.211%<br>Execution Time: 0.3364 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 90.000%<br>Execution Time: 0.9370 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 96.667%<br>Execution Time: 0.0356 seconds</p>
</blockquote>
<p><strong>运行结果</strong>(经后剪枝)</p>
<blockquote>
<p>Dataset <strong><code>bal</code></strong>, average accuracy: 72.321%<br>Execution Time: 0.2086 seconds<br>Dataset <strong><code>gla</code></strong>, average accuracy: 43.684%<br>Execution Time: 1.3278 seconds<br>Dataset <strong><code>hay</code></strong>, average accuracy: 70.000%<br>Execution Time: 0.0362 seconds<br>Dataset <strong><code>iri</code></strong>, average accuracy: 84.615%<br>Execution Time: 0.0785 seconds<br>Dataset <strong><code>new</code></strong>, average accuracy: 81.579%<br>Execution Time: 0.2871 seconds<br>Dataset <strong><code>win</code></strong>, average accuracy: 71.250%<br>Execution Time: 0.8062 seconds<br>Dataset <strong><code>zoo</code></strong>, average accuracy: 61.111%<br>Execution Time: 0.0316 seconds</p>
</blockquote>
<h2 id="4-复盘与分析"><a href="#4-复盘与分析" class="headerlink" title="4.复盘与分析"></a>4.复盘与分析</h2><h3 id="可视化效果"><a href="#可视化效果" class="headerlink" title="可视化效果"></a>可视化效果</h3><p><img src="https://raw.githubusercontent.com/CPhoenixW/blog/refs/heads/gh-pages/images/1-1.png" alt="1-1"></p>
<h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p>在CART算法中，将数据特征值当作连续的值先进行排序，后分段可能运用到了数据的连续特征信息，提高了识别的分类的准确率。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-第一周学习周报"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/12/01/%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/"
    >第一周学习周报</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/12/01/%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E6%8A%A5/" class="article-date">
  <time datetime="2024-11-30T16:00:00.000Z" itemprop="datePublished">2024-12-01</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="K近邻算法（KNN）"><a href="#K近邻算法（KNN）" class="headerlink" title="K近邻算法（KNN）"></a>K近邻算法（KNN）</h1><h2 id="1-数据输入"><a href="#1-数据输入" class="headerlink" title="1.数据输入"></a>1.数据输入</h2><p>输入所有数据集的特征向量与标签。</p>
<p>10次10折交叉验证，将数据集划分为训练集和测试集。</p>
<h2 id="2-算法预测"><a href="#2-算法预测" class="headerlink" title="2.算法预测"></a>2.算法预测</h2><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><p>两种距离计算方式：</p>
<p><strong>欧几里得距离</strong><br>$$<br>𝑑(𝑥,𝑦)&#x3D;\sqrt {\sum_{i&#x3D;1}^n (x_i-y_i)^2}<br>$$<br><strong>曼哈顿距离</strong><br>$$<br>d(x,y)&#x3D;\sum_{i&#x3D;1}^n \mid x_i -y_i \mid<br>$$</p>
<h3 id="顺序排列"><a href="#顺序排列" class="headerlink" title="顺序排列"></a>顺序排列</h3><p>根据之前计算的距离，选择k个距离最小的项进行计分。</p>
<h3 id="多数投票法"><a href="#多数投票法" class="headerlink" title="多数投票法"></a>多数投票法</h3><p>根据标签与计分表进行投票。判断最相似的结果。</p>
<h2 id="3-算法优化"><a href="#3-算法优化" class="headerlink" title="3.算法优化"></a>3.算法优化</h2><p>当训练集很大时，对顺序计算预测量与每一个标记点的距离非常耗时，采用<strong>KD树</strong><code>(K-dimensional tree)</code>提高效率。</p>
<h3 id="构建KD树"><a href="#构建KD树" class="headerlink" title="构建KD树"></a>构建KD树</h3><p><strong>1.选择一个维度</strong>，维度的选择通过循环决定（例如，选择第1维、2维、3维等，循环使用）来决定每一层分割的维度。</p>
<p><strong>2.选择分割点</strong>，将训练集中所选维度中位数定义为分割点，小于中位数归类于左子树，反之归类于右子树。</p>
<p><strong>3.递归分割</strong>，将左右两部分递归地进行相同的操作，直到分割到叶节点。</p>
<h3 id="查询最近邻"><a href="#查询最近邻" class="headerlink" title="查询最近邻"></a><strong>查询最近邻</strong></h3><p><strong>1.遍历树</strong>，从树的根节点开始，递归地遍历树。</p>
<p><strong>2.更新最短距离</strong>，若查询点到当前子树的根节点距离小于最小距离，更新最小距离。</p>
<p><strong>3.选择左子树或右子树</strong>，根据查询点的坐标与当前节点的分割维度进行比较，决定是否向左子树或右子树搜索。</p>
<ul>
<li>如果查询点再该维度下的坐标小于当前节点的分割点，则搜索左子树；反之，搜索右子树。</li>
</ul>
<p><strong>4.回溯并搜索另一子树</strong>，一旦进入一个子树，需要回溯并继续搜索另一子树。</p>
<p><strong>5.剪枝</strong>，如果当前子树中的端点到查询点的距离已经大于当前最短的距离，则跳过该子树的搜索。</p>
<h2 id="4-代码复现"><a href="#4-代码复现" class="headerlink" title="4.代码复现"></a>4.代码复现</h2><h3 id="全扫描版本"><a href="#全扫描版本" class="headerlink" title="全扫描版本"></a>全扫描版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span>(i%num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_vector, val_vector, train_label, val_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_distance</span>(<span class="params">train_vector, test_vector</span>):</span><br><span class="line">    distance = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_vector)):</span><br><span class="line">        count  = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">            count += (train_vector[i][j] - test_vector[j]) ** <span class="number">2</span></span><br><span class="line">        count = count ** <span class="number">0.5</span></span><br><span class="line">        distance.append(count)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sort_and_vote</span>(<span class="params">distance, train_label, k</span>):</span><br><span class="line">    vote_label = []</span><br><span class="line">    combine = <span class="built_in">list</span>(<span class="built_in">zip</span>(distance, train_label))</span><br><span class="line">    sort = <span class="built_in">sorted</span>(combine, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        vote_label.append(sort[i][<span class="number">1</span>])</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> vote_label:</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">in</span> label_count:</span><br><span class="line">            label_count[label] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label_count[label] = <span class="number">1</span></span><br><span class="line">    common = <span class="literal">None</span></span><br><span class="line">    <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> label, count <span class="keyword">in</span> label_count.items():</span><br><span class="line">        <span class="keyword">if</span> count &gt; <span class="built_in">max</span>:</span><br><span class="line">            common = label</span><br><span class="line">            <span class="built_in">max</span> = count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> common</span><br><span class="line"></span><br><span class="line">vector, label = read_excel(<span class="string">r&quot;E:\Document\MachineLearning\dataset\bal.xls&quot;</span>)</span><br><span class="line">start = time.perf_counter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_vector)):</span><br><span class="line">        test_vector = val_vector[j]</span><br><span class="line">        distance = cal_distance(train_vector, test_vector)</span><br><span class="line">        common = sort_and_vote(distance, train_label, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span>(common == val_label[j]):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation %d, accuracy: %.3f %%&quot;</span>%(i, count / <span class="built_in">len</span>(val_vector)*<span class="number">100</span>))</span><br><span class="line">end = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running spend %f s&quot;</span>%(end - start))</span><br></pre></td></tr></table></figure>

<h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果:"></a>运行结果:</h4><p><code>bal</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 92.063 %<br>Validation 1, accuracy: 79.365 %<br>Validation 2, accuracy: 79.365 %<br>Validation 3, accuracy: 85.714 %<br>Validation 4, accuracy: 77.419 %<br>Validation 5, accuracy: 74.194 %<br>Validation 6, accuracy: 82.258 %<br>Validation 7, accuracy: 80.645 %<br>Validation 8, accuracy: 79.032 %<br>Validation 9, accuracy: 82.258 %<br>Running spend 0.773550 s</p>
</blockquote>
<p><code>gla</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 68.182 %<br>Validation 1, accuracy: 68.182 %<br>Validation 2, accuracy: 72.727 %<br>Validation 3, accuracy: 57.143 %<br>Validation 4, accuracy: 61.905 %<br>Validation 5, accuracy: 76.190 %<br>Validation 6, accuracy: 76.190 %<br>Validation 7, accuracy: 66.667 %<br>Validation 8, accuracy: 71.429 %<br>Validation 9, accuracy: 76.190 %<br>Running spend 0.203716 s</p>
</blockquote>
<p><code>hay</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 68.750 %<br>Validation 1, accuracy: 75.000 %<br>Validation 2, accuracy: 75.000 %<br>Validation 3, accuracy: 62.500 %<br>Validation 4, accuracy: 62.500 %<br>Validation 5, accuracy: 62.500 %<br>Validation 6, accuracy: 62.500 %<br>Validation 7, accuracy: 87.500 %<br>Validation 8, accuracy: 68.750 %<br>Validation 9, accuracy: 66.667 %<br>Running spend 0.116895 s</p>
</blockquote>
<p><code>iri</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 93.333 %<br>Validation 2, accuracy: 93.333 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 93.333 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 93.333 %<br>Validation 9, accuracy: 92.857 %<br>Running spend 0.052544 s</p>
</blockquote>
<p><code>new</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 90.909 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 90.909 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 90.476 %<br>Running spend 0.121106 s</p>
</blockquote>
<p><code>win</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 90.909 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 90.909 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 90.476 %<br>Running spend 0.121106 s</p>
</blockquote>
<p><code>zoo</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 90.000 %<br>Validation 1, accuracy: 90.000 %<br>Validation 2, accuracy: 100.000 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 90.000 %<br>Validation 5, accuracy: 90.000 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 100.000 %<br>Validation 9, accuracy: 80.000 %<br>Running spend 0.070432 s</p>
</blockquote>
<h3 id="KD搜索树版本"><a href="#KD搜索树版本" class="headerlink" title="KD搜索树版本"></a>KD搜索树版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_excel</span>(<span class="params">location</span>):</span><br><span class="line">    df = pd.read_excel(location)</span><br><span class="line">    vector = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">    label = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">    <span class="keyword">return</span> vector, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_division</span>(<span class="params">vector, label, num, c</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(label)</span><br><span class="line">    train_vector = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_vector = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span>(i%num != c):</span><br><span class="line">            train_vector.append(vector[i])</span><br><span class="line">            train_label.append(label[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            val_vector.append(vector[i])</span><br><span class="line">            val_label.append(label[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_vector, val_vector, train_label, val_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_distance</span>(<span class="params">train_vector, test_vector</span>):</span><br><span class="line">    distance = []</span><br><span class="line">    count  = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_vector)):</span><br><span class="line">        count += (train_vector[i] - test_vector[i]) ** <span class="number">2</span></span><br><span class="line">    count = count ** <span class="number">0.5</span></span><br><span class="line">    distance.append(count)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KD_Tree</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, point, label, left=<span class="literal">None</span>, right=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.point = point</span><br><span class="line">        <span class="variable language_">self</span>.label = label</span><br><span class="line">        <span class="variable language_">self</span>.left = left</span><br><span class="line">        <span class="variable language_">self</span>.right = right</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">vector, labels, depth=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(vector) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    k = <span class="built_in">len</span>(vector[<span class="number">0</span>])</span><br><span class="line">    i = depth % k</span><br><span class="line">    sort = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(vector, labels), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>][i])</span><br><span class="line">    m_index = <span class="built_in">len</span>(sort) // <span class="number">2</span></span><br><span class="line">    m_point, m_label = sort[m_index]</span><br><span class="line">    l_points = [point <span class="keyword">for</span> point, _ <span class="keyword">in</span> sort[:m_index]]</span><br><span class="line">    l_labels = [label <span class="keyword">for</span> _, label <span class="keyword">in</span> sort[:m_index]]</span><br><span class="line">    r_points = [point <span class="keyword">for</span> point, _ <span class="keyword">in</span> sort[m_index + <span class="number">1</span>:]]</span><br><span class="line">    r_labels = [label <span class="keyword">for</span> _, label <span class="keyword">in</span> sort[m_index + <span class="number">1</span>:]]</span><br><span class="line">    l_node = build_tree(l_points, l_labels, depth + <span class="number">1</span>)</span><br><span class="line">    r_node = build_tree(r_points, r_labels, depth + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> KD_Tree(m_point, m_label, l_node, r_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tree</span>(<span class="params">tree, test_vector, depth=<span class="number">0</span>, best=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line">    current_distance = cal_distance(tree.point, test_vector)</span><br><span class="line">    <span class="keyword">if</span> best <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> current_distance &lt; best[<span class="number">2</span>]:</span><br><span class="line">        best = (tree.point, tree.label, current_distance)</span><br><span class="line">    i = depth % <span class="built_in">len</span>(test_vector)</span><br><span class="line">    <span class="keyword">if</span> test_vector[i] &lt; tree.point[i]:</span><br><span class="line">        next_branch, other_branch= tree.left, tree.right</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_branch, other_branch= tree.right, tree.left</span><br><span class="line">    best = search_tree(next_branch, test_vector, depth + <span class="number">1</span>, best)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(test_vector[i] - tree.point[i]) &lt; best[<span class="number">2</span>]:</span><br><span class="line">        best = search_tree(other_branch, test_vector, depth + <span class="number">1</span>, best)</span><br><span class="line">    <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vector, label = read_excel(<span class="string">r&quot;E:\Document\MachineLearning\dataset\bal.xls&quot;</span>)</span><br><span class="line">start = time.perf_counter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train_vector, val_vector, train_label, val_label = dataset_division(vector, label, <span class="number">10</span>, i)</span><br><span class="line">    kd_tree = build_tree(train_vector, train_label)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(val_label)):</span><br><span class="line">        best = search_tree(kd_tree, val_vector[j])</span><br><span class="line">        <span class="keyword">if</span> best[<span class="number">1</span>] == val_label[j]:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation %d, accuracy: %.3f %%&quot;</span>%(i, count / <span class="built_in">len</span>(val_vector)*<span class="number">100</span>))</span><br><span class="line">end = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running spend %f s&quot;</span>%(end - start))</span><br></pre></td></tr></table></figure>

<h4 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h4><p><code>bal</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 87.302 %<br>Validation 1, accuracy: 82.540 %<br>Validation 2, accuracy: 79.365 %<br>Validation 3, accuracy: 80.952 %<br>Validation 4, accuracy: 82.258 %<br>Validation 5, accuracy: 74.194 %<br>Validation 6, accuracy: 75.806 %<br>Validation 7, accuracy: 88.710 %<br>Validation 8, accuracy: 82.258 %<br>Validation 9, accuracy: 74.194 %<br>Running spend 0.082670 s</p>
</blockquote>
<p><code>gla</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 63.636 %<br>Validation 1, accuracy: 68.182 %<br>Validation 2, accuracy: 72.727 %<br>Validation 3, accuracy: 71.429 %<br>Validation 4, accuracy: 61.905 %<br>Validation 5, accuracy: 76.190 %<br>Validation 6, accuracy: 76.190 %<br>Validation 7, accuracy: 80.952 %<br>Validation 8, accuracy: 80.952 %<br>Validation 9, accuracy: 76.190 %<br>Running spend 0.087868 s</p>
</blockquote>
<p><code>hay</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 81.250 %<br>Validation 1, accuracy: 75.000 %<br>Validation 2, accuracy: 81.250 %<br>Validation 3, accuracy: 43.750 %<br>Validation 4, accuracy: 81.250 %<br>Validation 5, accuracy: 75.000 %<br>Validation 6, accuracy: 43.750 %<br>Validation 7, accuracy: 68.750 %<br>Validation 8, accuracy: 62.500 %<br>Validation 9, accuracy: 73.333 %<br>Running spend 0.021925 s</p>
</blockquote>
<p><code>iri</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 93.333 %<br>Validation 2, accuracy: 86.667 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 93.333 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 93.333 %<br>Validation 9, accuracy: 92.857 %<br>Running spend 0.021278 s</p>
</blockquote>
<p><code>new</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 100.000 %<br>Validation 1, accuracy: 95.455 %<br>Validation 2, accuracy: 95.455 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 95.238 %<br>Validation 5, accuracy: 95.238 %<br>Validation 6, accuracy: 95.238 %<br>Validation 7, accuracy: 95.238 %<br>Validation 8, accuracy: 85.714 %<br>Validation 9, accuracy: 80.952 %<br>Running spend 0.055435 s</p>
</blockquote>
<p><code>win</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 72.222 %<br>Validation 1, accuracy: 72.222 %<br>Validation 2, accuracy: 72.222 %<br>Validation 3, accuracy: 66.667 %<br>Validation 4, accuracy: 88.889 %<br>Validation 5, accuracy: 88.889 %<br>Validation 6, accuracy: 83.333 %<br>Validation 7, accuracy: 76.471 %<br>Validation 8, accuracy: 76.471 %<br>Validation 9, accuracy: 76.471 %<br>Running spend 0.200047 s</p>
</blockquote>
<p><code>zoo</code>数据集</p>
<blockquote>
<p>Validation 0, accuracy: 90.000 %<br>Validation 1, accuracy: 90.000 %<br>Validation 2, accuracy: 100.000 %<br>Validation 3, accuracy: 100.000 %<br>Validation 4, accuracy: 100.000 %<br>Validation 5, accuracy: 90.000 %<br>Validation 6, accuracy: 100.000 %<br>Validation 7, accuracy: 100.000 %<br>Validation 8, accuracy: 100.000 %<br>Validation 9, accuracy: 100.000 %<br>Running spend 0.049036 s</p>
</blockquote>
<h3 id="分析与复盘"><a href="#分析与复盘" class="headerlink" title="分析与复盘"></a>分析与复盘</h3><p>在使用<code>bal</code>数据集时，使用KD树进行搜索节省了90%左右的时间成本，但不知道为啥在validation 1，3，7，8中使用有局限性的KD搜索识别准确率竟然高于全局搜索，挺神奇的。（之所以选择<code>bal</code>数据集是由于他的维度较少，数量较多，能更好发挥KD树的优势）</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-241119"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/11/20/241119/"
    >24-11-19 log</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/11/20/241119/" class="article-date">
  <time datetime="2024-11-20T08:00:00.000Z" itemprop="datePublished">2024-11-20</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>昨天和今天勉勉强强在<em>ChatGPT</em>和一个<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ts4y1f7Gu/">B站视频</a>的帮助下写了一个静态的网站，然后部署到了<em>GitHub</em>上。虽然效果不尽人意，不知道为啥引用人家主题的时候在我本地运行的好好的，到了<em>GitHub</em>上就格式有问题。然而出人意料的是，到今天我写这个log文件的时候格式又好了，不知道是不是没有刷新导致的问题。<a href="https://cphoenixw.github.io/blog/">My Blog</a></p>
<p>我用的是一个 <em>Hexo</em> 开发框架，然后用了它 ayer 的一个主题。</p>
<p>下面是我的具体操作：（<strong>开发者模式</strong>）</p>
<p>0.调整 Git 配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;你的用户名&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;你的邮箱&quot;</span></span><br><span class="line"><span class="built_in">set</span> http_proxy=http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line"><span class="built_in">set</span> https_proxy=http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line">git config --global https.proxy http://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br><span class="line">git config --global https.proxy https://127.0.0.1:<span class="string">&quot;你的端口号&quot;</span></span><br></pre></td></tr></table></figure>

<p>1.安装 <em>Hexo</em>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /d <span class="string">&quot;项目地址&quot;</span></span><br><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure>

<p>2.初始化项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init <span class="string">&quot;新项目名称&quot;</span></span><br><span class="line"><span class="built_in">cd</span> <span class="string">&quot;新项目名称&quot;</span></span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>

<p>3.在 <code>source/_posts</code> 文件夹中创建 Markdown 文件：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">Copy code</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line">title: 文章标题</span><br><span class="line">date: 2024-11-18</span><br><span class="line"><span class="section">tags:</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line">这里是正文内容。</span><br></pre></td></tr></table></figure>

<p>4.安装主题：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/Shen-Yu/hexo-theme-ayer.git themes/ayer</span><br></pre></td></tr></table></figure>

<p>5.GitHub 网页端配置</p>
<p>登录 GitHub。<br>点击右上角 + 按钮，选择 New Repository。<br>创建一个新仓库，例如 blog。</p>
<p>6.配置 ssh:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">&quot;你的邮箱&quot;</span></span><br></pre></td></tr></table></figure>

<p>按照 cmd 提示获取 ssh 密钥，用 edge 打开即可。</p>
<p>登录<em>GitHub</em>主页( <strong>不是项目主页</strong> )，进入 Settings &gt; SSH and GPG keys，点击 New SSH key，粘贴密钥；</p>
<p>在 Code &gt; SSH 中复制SSH地址</p>
<p>7.修改配置，根目录下的<code> _config.yml</code>里</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">ayer</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">    <span class="attr">repo:</span> <span class="string">&quot;ssh链接&quot;</span></span><br><span class="line">    <span class="attr">branch:</span> <span class="string">gh-pages</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>8.测试 SSH：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>

<p>返回值为Hi &lt;你的用户名&gt;!xxx就好了</p>
<p>9.部署:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>

<p>10.访问网站</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/happy/" rel="tag">happy!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-hello-world"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/blog/2024/11/18/hello-world/"
    >Hello World!</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/blog/2024/11/18/hello-world/" class="article-date">
  <time datetime="2024-11-18T14:02:59.000Z" itemprop="datePublished">2024-11-18</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>Welcome to PhoenixW’s blog! This is his first post.<br>刚刚在本地部署了一下博客，效果不错。<br>然而部署到github上的时候不知道为啥主题就加载不出来了。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/testing/" rel="tag">testing!</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2024-2025
        <i class="ri-heart-fill heart_icon"></i> Phoenix W
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/blog/"><img src="./images/ayer-side.svg" alt="Phoenix W"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/blog">主页</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/blog/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/blog/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/blog/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/blog/js/jquery-3.6.0.min.js"></script>
 
<script src="/blog/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/blog/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->
 
    
        <link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.css">
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.js"></script>
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/contrib/auto-render.min.js"></script>
        
    
 
<!-- busuanzi  -->
 
<script src="/blog/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->

<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>